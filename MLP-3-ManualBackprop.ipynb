{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c857f5ef-9847-42bc-9b14-f16c99c011fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a5af913-949d-4c81-892b-995cca2dc4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n",
      "15\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
     ]
    }
   ],
   "source": [
    "# read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "print(len(words))\n",
    "print(max(len(w) for w in words))\n",
    "print(words[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "193ac9ca-9a1d-45b1-a5ea-7f72c63ccbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "447b4c90-3e05-41f9-a789-c6ff4263e023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec022e9b-379a-46fe-b9aa-f7fbc821ac11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
    "def cmp(s, dt, t):\n",
    "  ex = torch.all(dt == t.grad).item()\n",
    "  app = torch.allclose(dt, t.grad)\n",
    "  maxdiff = (dt - t.grad).abs().max().item()\n",
    "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b717c58-9519-44cf-a87f-7795ce6aceaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "# Note: I am initializating many of these parameters in non-standard ways\n",
    "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
    "# implementation of the backward pass.\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77ca85dc-cfd6-43de-b3f4-9b870bdf44ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size # a shorter variable also, for convenience\n",
    "# construct a minibatch\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dd31d4d-94da-4577-92da-912b114ae812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3390, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
    "\n",
    "emb = C[Xb] # embed the characters into vectors\n",
    "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "# Linear layer 1\n",
    "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "# BatchNorm layer\n",
    "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "# Non-linearity\n",
    "h = torch.tanh(hpreact) # hidden layer\n",
    "# Linear layer 2\n",
    "logits = h @ W2 + b2 # output layer\n",
    "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdims=True)\n",
    "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# PyTorch backward pass\n",
    "for p in parameters:\n",
    "  p.grad = None\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
    "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
    "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
    "         embcat, emb]:\n",
    "  t.retain_grad()\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "367953dd-169e-4ffc-9722-0b762c849dbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]),\n",
       " tensor([[-2.7014, -2.4817, -3.9915, -2.9118, -3.9038, -2.5258, -3.6007, -3.2637,\n",
       "          -3.9269, -3.4777, -3.2829, -3.3038, -3.2816, -3.5045, -3.4553, -4.3187,\n",
       "          -4.7335, -3.9371, -4.1697, -2.9211, -3.0176, -3.8755, -3.6935, -2.5567,\n",
       "          -2.8598, -3.6871, -3.8497],\n",
       "         [-2.9303, -2.8797, -2.4326, -2.8539, -3.2855, -3.3005, -3.9704, -3.0994,\n",
       "          -3.8292, -3.7310, -2.9420, -3.1105, -3.1436, -3.5209, -3.0645, -3.2504,\n",
       "          -3.6350, -4.0844, -3.9040, -3.4409, -3.9747, -3.8770, -4.3407, -2.6424,\n",
       "          -3.7022, -3.3207, -3.6607],\n",
       "         [-4.0226, -3.7113, -4.2520, -4.2274, -3.7668, -3.1186, -2.6889, -2.8739,\n",
       "          -2.8136, -3.4965, -3.8312, -3.3322, -3.1222, -2.9991, -3.8627, -3.6193,\n",
       "          -4.3266, -3.4233, -3.5865, -2.2730, -2.7595, -3.3092, -3.2443, -3.1995,\n",
       "          -3.1487, -3.9406, -3.5355],\n",
       "         [-3.5084, -3.6161, -3.2272, -2.8739, -2.7549, -3.4570, -3.0599, -3.1999,\n",
       "          -2.9283, -3.9868, -3.2112, -3.7078, -3.4480, -3.1297, -2.8584, -2.7518,\n",
       "          -3.7392, -3.6727, -4.0605, -2.9627, -4.0707, -3.9132, -3.3869, -3.7204,\n",
       "          -3.5581, -3.0966, -3.1293],\n",
       "         [-4.1794, -4.0711, -3.7117, -3.7249, -4.0403, -3.1769, -3.2334, -4.0795,\n",
       "          -2.8592, -3.2328, -3.4046, -4.0674, -3.1959, -3.7140, -3.1536, -1.7594,\n",
       "          -4.0281, -2.9564, -3.0162, -2.8437, -3.2425, -4.2366, -3.9399, -4.1589,\n",
       "          -3.0584, -3.8482, -3.1634],\n",
       "         [-3.3525, -3.3291, -2.9776, -2.6792, -2.6529, -3.5042, -3.4744, -2.9311,\n",
       "          -3.5629, -4.1181, -3.0705, -3.9877, -3.6580, -3.1343, -2.7752, -3.0162,\n",
       "          -4.1195, -3.8439, -4.5368, -3.5248, -3.4992, -3.6524, -3.2489, -3.9068,\n",
       "          -3.9653, -2.5495, -3.1096],\n",
       "         [-2.9107, -4.4029, -2.7342, -4.1779, -3.4207, -3.8973, -2.7031, -2.9157,\n",
       "          -2.9820, -3.0863, -3.9608, -3.2691, -3.6495, -3.9479, -4.1841, -2.6805,\n",
       "          -2.5613, -3.1296, -4.5773, -3.5206, -3.6365, -3.0229, -3.1725, -3.7627,\n",
       "          -3.1822, -3.7498, -3.4924],\n",
       "         [-3.1362, -3.4859, -3.7526, -2.1293, -3.5537, -2.9471, -3.4327, -2.4616,\n",
       "          -3.2380, -4.7574, -3.5352, -4.2187, -3.9263, -2.8370, -4.0139, -4.1932,\n",
       "          -4.3323, -4.2863, -4.2316, -2.4552, -3.4019, -3.0674, -3.6839, -3.3220,\n",
       "          -3.7187, -2.9166, -3.3540],\n",
       "         [-3.0727, -3.5586, -4.0738, -3.2518, -4.4700, -3.2054, -2.8315, -2.3907,\n",
       "          -3.4883, -3.8671, -3.4944, -3.4088, -3.2594, -3.6692, -4.5785, -4.0267,\n",
       "          -3.9785, -3.2684, -3.8448, -2.4603, -3.3430, -2.4793, -3.1712, -3.0914,\n",
       "          -3.1643, -4.1274, -3.4540],\n",
       "         [-3.6026, -4.1721, -3.6071, -3.9480, -3.4449, -3.2530, -3.3090, -3.5611,\n",
       "          -2.3395, -2.7802, -4.2407, -3.2462, -3.2736, -3.1624, -3.7452, -3.5709,\n",
       "          -2.8298, -3.0653, -3.1662, -3.7754, -2.7041, -3.7061, -2.8129, -3.8919,\n",
       "          -3.3993, -3.3216, -3.9372],\n",
       "         [-4.2278, -4.7971, -3.2895, -3.1107, -3.4338, -3.2690, -3.8746, -5.0834,\n",
       "          -3.2463, -3.0725, -3.6806, -3.9138, -2.8224, -3.4644, -3.3378, -1.9701,\n",
       "          -3.0216, -2.8229, -2.5774, -3.2919, -3.1366, -4.5971, -4.3831, -3.7672,\n",
       "          -3.2222, -3.5345, -3.7950],\n",
       "         [-3.8650, -3.5048, -3.8732, -1.6348, -4.4470, -2.8134, -2.6624, -3.1204,\n",
       "          -3.0938, -3.4792, -4.2022, -3.9079, -3.8743, -3.5979, -5.0735, -5.0171,\n",
       "          -3.9220, -3.3558, -4.1200, -2.8804, -2.9981, -3.3764, -4.0422, -3.2071,\n",
       "          -3.0404, -3.1513, -4.1615],\n",
       "         [-3.9986, -4.2186, -3.1129, -4.1872, -3.5745, -4.1099, -2.4259, -3.3320,\n",
       "          -2.7481, -2.9668, -4.0285, -3.2393, -3.2317, -3.4695, -4.1801, -3.1506,\n",
       "          -2.6489, -3.0627, -3.2218, -3.8896, -3.0603, -3.2755, -2.4394, -4.4870,\n",
       "          -3.7543, -3.6881, -3.5732],\n",
       "         [-3.8318, -3.5319, -3.0222, -3.8118, -4.0817, -2.5854, -3.5101, -4.2342,\n",
       "          -3.4859, -3.0853, -3.4831, -3.4053, -2.6776, -3.3022, -3.0293, -3.0506,\n",
       "          -3.5766, -3.3871, -2.5009, -4.2808, -2.7787, -4.1759, -4.1284, -3.2669,\n",
       "          -3.2716, -3.3270, -3.2968],\n",
       "         [-3.1364, -3.6514, -3.6242, -2.8890, -3.9786, -3.2900, -3.3974, -2.8752,\n",
       "          -2.8308, -2.9730, -3.4004, -3.0699, -2.9908, -3.1926, -3.6294, -3.7035,\n",
       "          -3.8899, -2.9276, -3.5533, -3.1440, -2.9819, -3.1304, -3.7530, -3.2478,\n",
       "          -3.7286, -3.8658, -3.7855],\n",
       "         [-3.1885, -2.7869, -3.4759, -3.9506, -3.8852, -2.4886, -3.4403, -3.8116,\n",
       "          -3.4985, -2.9442, -3.2817, -3.0223, -2.8298, -4.1559, -3.5521, -4.0556,\n",
       "          -3.2283, -3.9614, -2.8780, -4.1055, -3.3369, -3.6961, -3.4962, -2.6574,\n",
       "          -2.8533, -3.5911, -3.8169],\n",
       "         [-3.6111, -3.7083, -2.5289, -3.0972, -2.7286, -2.8863, -4.1045, -3.6033,\n",
       "          -3.8707, -3.9983, -2.9423, -3.8768, -3.2329, -2.8504, -2.7473, -2.7644,\n",
       "          -4.2497, -4.0868, -3.7638, -3.1215, -3.6762, -4.2298, -4.7960, -2.9534,\n",
       "          -3.6867, -3.1466, -2.9009],\n",
       "         [-3.0283, -2.5339, -3.0518, -2.9079, -3.1920, -3.3156, -3.8754, -2.6162,\n",
       "          -3.5724, -4.0970, -2.5920, -4.2659, -3.7885, -3.9197, -3.1693, -3.5118,\n",
       "          -4.2752, -3.5320, -4.1161, -3.0407, -3.8668, -3.1209, -3.7240, -3.8591,\n",
       "          -3.6661, -3.1752, -2.7136],\n",
       "         [-3.9986, -4.2186, -3.1129, -4.1872, -3.5745, -4.1099, -2.4259, -3.3320,\n",
       "          -2.7481, -2.9668, -4.0285, -3.2393, -3.2317, -3.4695, -4.1801, -3.1506,\n",
       "          -2.6489, -3.0627, -3.2218, -3.8896, -3.0603, -3.2755, -2.4394, -4.4870,\n",
       "          -3.7543, -3.6881, -3.5732],\n",
       "         [-3.8492, -3.8208, -3.4387, -4.0089, -3.2687, -3.3818, -2.5606, -3.0376,\n",
       "          -3.3129, -3.3694, -3.4743, -3.1000, -2.8357, -2.8687, -4.3165, -3.7149,\n",
       "          -3.5544, -3.9637, -3.5225, -2.5351, -3.0457, -3.3795, -3.2352, -2.9846,\n",
       "          -3.3886, -3.8409, -3.6443],\n",
       "         [-2.9493, -4.2431, -4.0950, -3.1885, -3.4741, -3.2201, -3.4717, -3.2531,\n",
       "          -2.9115, -3.8862, -2.6745, -3.2913, -2.8041, -3.8064, -3.1796, -2.6011,\n",
       "          -4.1283, -3.8472, -3.3053, -2.8691, -4.0119, -3.6654, -3.8320, -3.6517,\n",
       "          -3.2290, -2.9139, -3.2438],\n",
       "         [-3.0263, -2.6694, -2.9886, -3.0078, -3.6577, -2.7761, -3.4259, -3.1071,\n",
       "          -3.6272, -4.1058, -2.8920, -3.6270, -3.6941, -3.4232, -3.2645, -3.0842,\n",
       "          -4.0358, -3.2856, -3.9413, -2.4507, -4.3656, -3.4998, -4.2911, -3.4021,\n",
       "          -3.3707, -3.5086, -3.3357],\n",
       "         [-3.9986, -4.2186, -3.1129, -4.1872, -3.5745, -4.1099, -2.4259, -3.3320,\n",
       "          -2.7481, -2.9668, -4.0285, -3.2393, -3.2317, -3.4695, -4.1801, -3.1506,\n",
       "          -2.6489, -3.0627, -3.2218, -3.8896, -3.0603, -3.2755, -2.4394, -4.4870,\n",
       "          -3.7543, -3.6881, -3.5732],\n",
       "         [-3.1157, -3.9437, -3.7212, -4.3112, -2.8185, -3.6392, -2.6625, -3.4692,\n",
       "          -3.6280, -3.2168, -3.3969, -3.2325, -3.3461, -3.3322, -3.3510, -3.0187,\n",
       "          -2.5982, -3.8120, -3.2912, -4.2079, -4.0193, -3.8779, -2.2682, -3.9380,\n",
       "          -2.8237, -3.5110, -4.0265],\n",
       "         [-3.4287, -2.5372, -3.7643, -3.0347, -3.6083, -2.7193, -3.7047, -3.4912,\n",
       "          -3.5158, -3.5650, -2.2802, -3.9898, -3.5335, -4.4041, -3.3118, -4.0284,\n",
       "          -4.1451, -4.1731, -4.4012, -3.6700, -4.3572, -3.8302, -4.5584, -1.8531,\n",
       "          -3.0289, -3.6462, -3.0069],\n",
       "         [-2.7958, -3.4990, -4.3046, -3.9010, -3.4454, -2.5263, -3.1732, -3.4855,\n",
       "          -3.5120, -3.3684, -3.4627, -2.8877, -2.9391, -3.3415, -3.9208, -4.3171,\n",
       "          -3.4405, -3.8966, -2.9152, -4.3707, -3.6849, -3.7784, -2.6785, -2.9118,\n",
       "          -2.6018, -3.1617, -4.4969],\n",
       "         [-3.2000, -3.6819, -3.9658, -4.5191, -3.0512, -3.8518, -2.4073, -3.2374,\n",
       "          -3.4229, -2.9778, -4.0361, -3.0577, -2.8187, -3.4684, -3.6873, -3.9489,\n",
       "          -2.8356, -3.6353, -3.5043, -3.4852, -3.8216, -3.3425, -2.0917, -3.9306,\n",
       "          -3.1995, -3.6637, -4.5629],\n",
       "         [-3.6452, -3.4870, -3.1062, -3.8520, -3.3616, -3.6984, -3.2515, -3.6109,\n",
       "          -3.1568, -2.9257, -3.7886, -3.4742, -3.2785, -3.3163, -3.1395, -2.7217,\n",
       "          -3.1437, -3.0235, -2.9831, -4.1903, -2.3795, -4.2624, -3.1755, -4.3178,\n",
       "          -3.9620, -3.4112, -3.0385],\n",
       "         [-2.7296, -3.1959, -4.2246, -3.8799, -3.4101, -2.7933, -3.0642, -3.2775,\n",
       "          -4.4649, -3.6706, -3.0969, -3.4894, -3.6815, -3.8647, -3.6895, -3.4310,\n",
       "          -2.6486, -3.6590, -2.7610, -3.7516, -3.4086, -3.4894, -3.1283, -2.7994,\n",
       "          -2.5169, -4.3937, -3.9534],\n",
       "         [-3.8964, -4.0220, -3.5177, -3.3518, -2.9508, -3.7135, -2.6509, -3.4392,\n",
       "          -2.5815, -3.4684, -3.4944, -3.3355, -3.4920, -3.2297, -3.4691, -2.7153,\n",
       "          -3.5454, -3.5791, -3.3028, -3.5494, -3.1869, -3.7354, -3.0146, -4.1377,\n",
       "          -3.3306, -2.9807, -3.2548],\n",
       "         [-3.3309, -3.0204, -3.8140, -2.7136, -3.1006, -2.9412, -3.2661, -3.4822,\n",
       "          -3.4931, -3.9442, -3.2853, -3.8059, -4.0324, -3.6322, -4.5228, -3.4408,\n",
       "          -3.4317, -3.5935, -3.1779, -3.0060, -2.9051, -3.6049, -3.8625, -2.5533,\n",
       "          -2.4685, -3.8789, -3.6227],\n",
       "         [-3.0813, -3.6796, -2.8133, -3.9358, -2.5339, -3.4864, -4.0580, -3.7407,\n",
       "          -4.1308, -2.8550, -3.5802, -3.4333, -3.4452, -2.5062, -3.2250, -3.2201,\n",
       "          -2.9864, -3.1445, -3.1617, -4.3765, -2.9072, -4.6666, -2.8577, -3.7194,\n",
       "          -3.8304, -3.3227, -3.9386]], grad_fn=<IndexBackward0>))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logprobs[range(n)].shape, logprobs[range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a6896d8-594c-4d7c-8764-94b8f79b0575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]),\n",
       " tensor([[-2.7014, -2.4817, -3.9915, -2.9118, -3.9038, -2.5258, -3.6007, -3.2637,\n",
       "          -3.9269, -3.4777, -3.2829, -3.3038, -3.2816, -3.5045, -3.4553, -4.3187,\n",
       "          -4.7335, -3.9371, -4.1697, -2.9211, -3.0176, -3.8755, -3.6935, -2.5567,\n",
       "          -2.8598, -3.6871, -3.8497],\n",
       "         [-2.9303, -2.8797, -2.4326, -2.8539, -3.2855, -3.3005, -3.9704, -3.0994,\n",
       "          -3.8292, -3.7310, -2.9420, -3.1105, -3.1436, -3.5209, -3.0645, -3.2504,\n",
       "          -3.6350, -4.0844, -3.9040, -3.4409, -3.9747, -3.8770, -4.3407, -2.6424,\n",
       "          -3.7022, -3.3207, -3.6607],\n",
       "         [-4.0226, -3.7113, -4.2520, -4.2274, -3.7668, -3.1186, -2.6889, -2.8739,\n",
       "          -2.8136, -3.4965, -3.8312, -3.3322, -3.1222, -2.9991, -3.8627, -3.6193,\n",
       "          -4.3266, -3.4233, -3.5865, -2.2730, -2.7595, -3.3092, -3.2443, -3.1995,\n",
       "          -3.1487, -3.9406, -3.5355],\n",
       "         [-3.5084, -3.6161, -3.2272, -2.8739, -2.7549, -3.4570, -3.0599, -3.1999,\n",
       "          -2.9283, -3.9868, -3.2112, -3.7078, -3.4480, -3.1297, -2.8584, -2.7518,\n",
       "          -3.7392, -3.6727, -4.0605, -2.9627, -4.0707, -3.9132, -3.3869, -3.7204,\n",
       "          -3.5581, -3.0966, -3.1293],\n",
       "         [-4.1794, -4.0711, -3.7117, -3.7249, -4.0403, -3.1769, -3.2334, -4.0795,\n",
       "          -2.8592, -3.2328, -3.4046, -4.0674, -3.1959, -3.7140, -3.1536, -1.7594,\n",
       "          -4.0281, -2.9564, -3.0162, -2.8437, -3.2425, -4.2366, -3.9399, -4.1589,\n",
       "          -3.0584, -3.8482, -3.1634],\n",
       "         [-3.3525, -3.3291, -2.9776, -2.6792, -2.6529, -3.5042, -3.4744, -2.9311,\n",
       "          -3.5629, -4.1181, -3.0705, -3.9877, -3.6580, -3.1343, -2.7752, -3.0162,\n",
       "          -4.1195, -3.8439, -4.5368, -3.5248, -3.4992, -3.6524, -3.2489, -3.9068,\n",
       "          -3.9653, -2.5495, -3.1096],\n",
       "         [-2.9107, -4.4029, -2.7342, -4.1779, -3.4207, -3.8973, -2.7031, -2.9157,\n",
       "          -2.9820, -3.0863, -3.9608, -3.2691, -3.6495, -3.9479, -4.1841, -2.6805,\n",
       "          -2.5613, -3.1296, -4.5773, -3.5206, -3.6365, -3.0229, -3.1725, -3.7627,\n",
       "          -3.1822, -3.7498, -3.4924],\n",
       "         [-3.1362, -3.4859, -3.7526, -2.1293, -3.5537, -2.9471, -3.4327, -2.4616,\n",
       "          -3.2380, -4.7574, -3.5352, -4.2187, -3.9263, -2.8370, -4.0139, -4.1932,\n",
       "          -4.3323, -4.2863, -4.2316, -2.4552, -3.4019, -3.0674, -3.6839, -3.3220,\n",
       "          -3.7187, -2.9166, -3.3540],\n",
       "         [-3.0727, -3.5586, -4.0738, -3.2518, -4.4700, -3.2054, -2.8315, -2.3907,\n",
       "          -3.4883, -3.8671, -3.4944, -3.4088, -3.2594, -3.6692, -4.5785, -4.0267,\n",
       "          -3.9785, -3.2684, -3.8448, -2.4603, -3.3430, -2.4793, -3.1712, -3.0914,\n",
       "          -3.1643, -4.1274, -3.4540],\n",
       "         [-3.6026, -4.1721, -3.6071, -3.9480, -3.4449, -3.2530, -3.3090, -3.5611,\n",
       "          -2.3395, -2.7802, -4.2407, -3.2462, -3.2736, -3.1624, -3.7452, -3.5709,\n",
       "          -2.8298, -3.0653, -3.1662, -3.7754, -2.7041, -3.7061, -2.8129, -3.8919,\n",
       "          -3.3993, -3.3216, -3.9372],\n",
       "         [-4.2278, -4.7971, -3.2895, -3.1107, -3.4338, -3.2690, -3.8746, -5.0834,\n",
       "          -3.2463, -3.0725, -3.6806, -3.9138, -2.8224, -3.4644, -3.3378, -1.9701,\n",
       "          -3.0216, -2.8229, -2.5774, -3.2919, -3.1366, -4.5971, -4.3831, -3.7672,\n",
       "          -3.2222, -3.5345, -3.7950],\n",
       "         [-3.8650, -3.5048, -3.8732, -1.6348, -4.4470, -2.8134, -2.6624, -3.1204,\n",
       "          -3.0938, -3.4792, -4.2022, -3.9079, -3.8743, -3.5979, -5.0735, -5.0171,\n",
       "          -3.9220, -3.3558, -4.1200, -2.8804, -2.9981, -3.3764, -4.0422, -3.2071,\n",
       "          -3.0404, -3.1513, -4.1615],\n",
       "         [-3.9986, -4.2186, -3.1129, -4.1872, -3.5745, -4.1099, -2.4259, -3.3320,\n",
       "          -2.7481, -2.9668, -4.0285, -3.2393, -3.2317, -3.4695, -4.1801, -3.1506,\n",
       "          -2.6489, -3.0627, -3.2218, -3.8896, -3.0603, -3.2755, -2.4394, -4.4870,\n",
       "          -3.7543, -3.6881, -3.5732],\n",
       "         [-3.8318, -3.5319, -3.0222, -3.8118, -4.0817, -2.5854, -3.5101, -4.2342,\n",
       "          -3.4859, -3.0853, -3.4831, -3.4053, -2.6776, -3.3022, -3.0293, -3.0506,\n",
       "          -3.5766, -3.3871, -2.5009, -4.2808, -2.7787, -4.1759, -4.1284, -3.2669,\n",
       "          -3.2716, -3.3270, -3.2968],\n",
       "         [-3.1364, -3.6514, -3.6242, -2.8890, -3.9786, -3.2900, -3.3974, -2.8752,\n",
       "          -2.8308, -2.9730, -3.4004, -3.0699, -2.9908, -3.1926, -3.6294, -3.7035,\n",
       "          -3.8899, -2.9276, -3.5533, -3.1440, -2.9819, -3.1304, -3.7530, -3.2478,\n",
       "          -3.7286, -3.8658, -3.7855],\n",
       "         [-3.1885, -2.7869, -3.4759, -3.9506, -3.8852, -2.4886, -3.4403, -3.8116,\n",
       "          -3.4985, -2.9442, -3.2817, -3.0223, -2.8298, -4.1559, -3.5521, -4.0556,\n",
       "          -3.2283, -3.9614, -2.8780, -4.1055, -3.3369, -3.6961, -3.4962, -2.6574,\n",
       "          -2.8533, -3.5911, -3.8169],\n",
       "         [-3.6111, -3.7083, -2.5289, -3.0972, -2.7286, -2.8863, -4.1045, -3.6033,\n",
       "          -3.8707, -3.9983, -2.9423, -3.8768, -3.2329, -2.8504, -2.7473, -2.7644,\n",
       "          -4.2497, -4.0868, -3.7638, -3.1215, -3.6762, -4.2298, -4.7960, -2.9534,\n",
       "          -3.6867, -3.1466, -2.9009],\n",
       "         [-3.0283, -2.5339, -3.0518, -2.9079, -3.1920, -3.3156, -3.8754, -2.6162,\n",
       "          -3.5724, -4.0970, -2.5920, -4.2659, -3.7885, -3.9197, -3.1693, -3.5118,\n",
       "          -4.2752, -3.5320, -4.1161, -3.0407, -3.8668, -3.1209, -3.7240, -3.8591,\n",
       "          -3.6661, -3.1752, -2.7136],\n",
       "         [-3.9986, -4.2186, -3.1129, -4.1872, -3.5745, -4.1099, -2.4259, -3.3320,\n",
       "          -2.7481, -2.9668, -4.0285, -3.2393, -3.2317, -3.4695, -4.1801, -3.1506,\n",
       "          -2.6489, -3.0627, -3.2218, -3.8896, -3.0603, -3.2755, -2.4394, -4.4870,\n",
       "          -3.7543, -3.6881, -3.5732],\n",
       "         [-3.8492, -3.8208, -3.4387, -4.0089, -3.2687, -3.3818, -2.5606, -3.0376,\n",
       "          -3.3129, -3.3694, -3.4743, -3.1000, -2.8357, -2.8687, -4.3165, -3.7149,\n",
       "          -3.5544, -3.9637, -3.5225, -2.5351, -3.0457, -3.3795, -3.2352, -2.9846,\n",
       "          -3.3886, -3.8409, -3.6443],\n",
       "         [-2.9493, -4.2431, -4.0950, -3.1885, -3.4741, -3.2201, -3.4717, -3.2531,\n",
       "          -2.9115, -3.8862, -2.6745, -3.2913, -2.8041, -3.8064, -3.1796, -2.6011,\n",
       "          -4.1283, -3.8472, -3.3053, -2.8691, -4.0119, -3.6654, -3.8320, -3.6517,\n",
       "          -3.2290, -2.9139, -3.2438],\n",
       "         [-3.0263, -2.6694, -2.9886, -3.0078, -3.6577, -2.7761, -3.4259, -3.1071,\n",
       "          -3.6272, -4.1058, -2.8920, -3.6270, -3.6941, -3.4232, -3.2645, -3.0842,\n",
       "          -4.0358, -3.2856, -3.9413, -2.4507, -4.3656, -3.4998, -4.2911, -3.4021,\n",
       "          -3.3707, -3.5086, -3.3357],\n",
       "         [-3.9986, -4.2186, -3.1129, -4.1872, -3.5745, -4.1099, -2.4259, -3.3320,\n",
       "          -2.7481, -2.9668, -4.0285, -3.2393, -3.2317, -3.4695, -4.1801, -3.1506,\n",
       "          -2.6489, -3.0627, -3.2218, -3.8896, -3.0603, -3.2755, -2.4394, -4.4870,\n",
       "          -3.7543, -3.6881, -3.5732],\n",
       "         [-3.1157, -3.9437, -3.7212, -4.3112, -2.8185, -3.6392, -2.6625, -3.4692,\n",
       "          -3.6280, -3.2168, -3.3969, -3.2325, -3.3461, -3.3322, -3.3510, -3.0187,\n",
       "          -2.5982, -3.8120, -3.2912, -4.2079, -4.0193, -3.8779, -2.2682, -3.9380,\n",
       "          -2.8237, -3.5110, -4.0265],\n",
       "         [-3.4287, -2.5372, -3.7643, -3.0347, -3.6083, -2.7193, -3.7047, -3.4912,\n",
       "          -3.5158, -3.5650, -2.2802, -3.9898, -3.5335, -4.4041, -3.3118, -4.0284,\n",
       "          -4.1451, -4.1731, -4.4012, -3.6700, -4.3572, -3.8302, -4.5584, -1.8531,\n",
       "          -3.0289, -3.6462, -3.0069],\n",
       "         [-2.7958, -3.4990, -4.3046, -3.9010, -3.4454, -2.5263, -3.1732, -3.4855,\n",
       "          -3.5120, -3.3684, -3.4627, -2.8877, -2.9391, -3.3415, -3.9208, -4.3171,\n",
       "          -3.4405, -3.8966, -2.9152, -4.3707, -3.6849, -3.7784, -2.6785, -2.9118,\n",
       "          -2.6018, -3.1617, -4.4969],\n",
       "         [-3.2000, -3.6819, -3.9658, -4.5191, -3.0512, -3.8518, -2.4073, -3.2374,\n",
       "          -3.4229, -2.9778, -4.0361, -3.0577, -2.8187, -3.4684, -3.6873, -3.9489,\n",
       "          -2.8356, -3.6353, -3.5043, -3.4852, -3.8216, -3.3425, -2.0917, -3.9306,\n",
       "          -3.1995, -3.6637, -4.5629],\n",
       "         [-3.6452, -3.4870, -3.1062, -3.8520, -3.3616, -3.6984, -3.2515, -3.6109,\n",
       "          -3.1568, -2.9257, -3.7886, -3.4742, -3.2785, -3.3163, -3.1395, -2.7217,\n",
       "          -3.1437, -3.0235, -2.9831, -4.1903, -2.3795, -4.2624, -3.1755, -4.3178,\n",
       "          -3.9620, -3.4112, -3.0385],\n",
       "         [-2.7296, -3.1959, -4.2246, -3.8799, -3.4101, -2.7933, -3.0642, -3.2775,\n",
       "          -4.4649, -3.6706, -3.0969, -3.4894, -3.6815, -3.8647, -3.6895, -3.4310,\n",
       "          -2.6486, -3.6590, -2.7610, -3.7516, -3.4086, -3.4894, -3.1283, -2.7994,\n",
       "          -2.5169, -4.3937, -3.9534],\n",
       "         [-3.8964, -4.0220, -3.5177, -3.3518, -2.9508, -3.7135, -2.6509, -3.4392,\n",
       "          -2.5815, -3.4684, -3.4944, -3.3355, -3.4920, -3.2297, -3.4691, -2.7153,\n",
       "          -3.5454, -3.5791, -3.3028, -3.5494, -3.1869, -3.7354, -3.0146, -4.1377,\n",
       "          -3.3306, -2.9807, -3.2548],\n",
       "         [-3.3309, -3.0204, -3.8140, -2.7136, -3.1006, -2.9412, -3.2661, -3.4822,\n",
       "          -3.4931, -3.9442, -3.2853, -3.8059, -4.0324, -3.6322, -4.5228, -3.4408,\n",
       "          -3.4317, -3.5935, -3.1779, -3.0060, -2.9051, -3.6049, -3.8625, -2.5533,\n",
       "          -2.4685, -3.8789, -3.6227],\n",
       "         [-3.0813, -3.6796, -2.8133, -3.9358, -2.5339, -3.4864, -4.0580, -3.7407,\n",
       "          -4.1308, -2.8550, -3.5802, -3.4333, -3.4452, -2.5062, -3.2250, -3.2201,\n",
       "          -2.9864, -3.1445, -3.1617, -4.3765, -2.9072, -4.6666, -2.8577, -3.7194,\n",
       "          -3.8304, -3.3227, -3.9386]], grad_fn=<LogBackward0>))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logprobs.shape, logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2acccc31-cfae-49c4-ba27-8999a274507c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32]),\n",
       " tensor([ 8, 14, 15, 22,  0, 19,  9, 14,  5,  1, 20,  3,  8, 14, 12,  0, 11,  0,\n",
       "         26,  9, 25,  0,  1,  1,  7, 18,  9,  3,  5,  9,  0, 18]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yb.shape, Yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36a847ae-e3c7-4081-b26e-93c7c96ec5d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32]),\n",
       " tensor([-3.9269, -3.0645, -3.6193, -3.3869, -4.1794, -3.5248, -3.0863, -4.0139,\n",
       "         -3.2054, -4.1721, -3.1366, -1.6348, -2.7481, -3.0293, -2.9908, -3.1885,\n",
       "         -3.8768, -3.0283, -3.5732, -3.3694, -2.9139, -3.0263, -4.2186, -3.9437,\n",
       "         -3.4912, -2.9152, -2.9778, -3.8520, -2.7933, -3.4684, -3.3309, -3.1617],\n",
       "        grad_fn=<IndexBackward0>))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logprobs[range(n), Yb].shape, logprobs[range(n), Yb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fb6f398-4f62-4e55-9a81-b80d3f12ec91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: backprop through the whole thing manually, \n",
    "# backpropagating through exactly all of the variables \n",
    "# as they are defined in the forward pass above, one by one\n",
    "\n",
    "dlogprobs = torch.zeros_like(logprobs) \n",
    "'''\n",
    "for example mean of a, b, c is\n",
    "mean = (a + b + c) / 3 (NLL)\n",
    "loss = -(a + b + c) / 3\n",
    "loss = -1/3a + -1/3b + -1/3c\n",
    "dloss/da = -1/3\n",
    "dloss/db = -1/3\n",
    "dloss/dc = -1/3\n",
    "\n",
    "Therefore, generally dloss/da = -1/n\n",
    "'''\n",
    "'''\n",
    "Additionaly we can see that logprobs is [32x27] and Yb is [32].\n",
    "But in loss we are not taking account of other values of logprobs and shrinking it to Yb i.e 32. So what happens to gradient of other \n",
    "values of logprobs?\n",
    "Well they don't matter as they don't contribute to the loss.\n",
    "'''\n",
    "dlogprobs[range(n), Yb] = -1.0/n #(Derivate of -logprobs[range(n), Yb].mean())\n",
    "dprobs = (1.0 / probs) * dlogprobs \n",
    "\n",
    "\n",
    "cmp('logprobs', dlogprobs, logprobs)\n",
    "# cmp('probs', dprobs, probs)\n",
    "# cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "# cmp('counts_sum', dcounts_sum, counts_sum)\n",
    "# cmp('counts', dcounts, counts)\n",
    "# cmp('norm_logits', dnorm_logits, norm_logits)\n",
    "# cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
    "# cmp('logits', dlogits, logits)\n",
    "# cmp('h', dh, h)\n",
    "# cmp('W2', dW2, W2)\n",
    "# cmp('b2', db2, b2)\n",
    "# cmp('hpreact', dhpreact, hpreact)\n",
    "# cmp('bngain', dbngain, bngain)\n",
    "# cmp('bnbias', dbnbias, bnbias)\n",
    "# cmp('bnraw', dbnraw, bnraw)\n",
    "# cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "# cmp('bnvar', dbnvar, bnvar)\n",
    "# cmp('bndiff2', dbndiff2, bndiff2)\n",
    "# cmp('bndiff', dbndiff, bndiff)\n",
    "# cmp('bnmeani', dbnmeani, bnmeani)\n",
    "# cmp('hprebn', dhprebn, hprebn)\n",
    "# cmp('embcat', dembcat, embcat)\n",
    "# cmp('W1', dW1, W1)\n",
    "# cmp('b1', db1, b1)\n",
    "# cmp('emb', demb, emb)\n",
    "# cmp('C', dC, C)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
