{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c857f5ef-9847-42bc-9b14-f16c99c011fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a5af913-949d-4c81-892b-995cca2dc4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n",
      "15\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
     ]
    }
   ],
   "source": [
    "# read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "print(len(words))\n",
    "print(max(len(w) for w in words))\n",
    "print(words[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "193ac9ca-9a1d-45b1-a5ea-7f72c63ccbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "447b4c90-3e05-41f9-a789-c6ff4263e023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec022e9b-379a-46fe-b9aa-f7fbc821ac11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
    "def cmp(s, dt, t):\n",
    "  ex = torch.all(dt == t.grad).item()\n",
    "  app = torch.allclose(dt, t.grad)\n",
    "  maxdiff = (dt - t.grad).abs().max().item()\n",
    "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b717c58-9519-44cf-a87f-7795ce6aceaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "# Note: I am initializating many of these parameters in non-standard ways\n",
    "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
    "# implementation of the backward pass.\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77ca85dc-cfd6-43de-b3f4-9b870bdf44ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size # a shorter variable also, for convenience\n",
    "# construct a minibatch\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dd31d4d-94da-4577-92da-912b114ae812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3392, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
    "\n",
    "emb = C[Xb] # embed the characters into vectors\n",
    "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "# Linear layer 1\n",
    "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "# BatchNorm layer\n",
    "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "# Non-linearity\n",
    "h = torch.tanh(hpreact) # hidden layer\n",
    "# Linear layer 2\n",
    "logits = h @ W2 + b2 # output layer\n",
    "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdims=True)\n",
    "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# PyTorch backward pass\n",
    "for p in parameters:\n",
    "  p.grad = None\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
    "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
    "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
    "         embcat, emb]:\n",
    "  t.retain_grad()\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "639e25a1-2263-4740-974f-2be84c27706c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]),\n",
       " torch.Size([32, 1]),\n",
       " torch.Size([32, 1]),\n",
       " torch.Size([32, 27]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.shape, counts_sum.shape, counts_sum_inv.shape, probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1fb6f398-4f62-4e55-9a81-b80d3f12ec91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hpreact         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bngain          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnbias          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnraw           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar_inv       | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff2         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnmeani         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hprebn          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "embcat          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "emb             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "C               | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: backprop through the whole thing manually, \n",
    "# backpropagating through exactly all of the variables \n",
    "# as they are defined in the forward pass above, one by one\n",
    "\n",
    "'''\n",
    "for example mean of a, b, c is\n",
    "mean = (a + b + c) / 3 (NLL)\n",
    "loss = -(a + b + c) / 3\n",
    "loss = -1/3a + -1/3b + -1/3c\n",
    "dloss/da = -1/3\n",
    "dloss/db = -1/3\n",
    "dloss/dc = -1/3\n",
    "\n",
    "Therefore, generally dloss/da = -1/n\n",
    "'''\n",
    "'''\n",
    "Additionaly we can see that logprobs is [32x27] and Yb is [32].\n",
    "But in loss we are not taking account of other values of logprobs and shrinking it to Yb i.e 32. So what happens to gradient of other \n",
    "values of logprobs?\n",
    "Well they don't matter as they don't contribute to the loss.\n",
    "'''\n",
    "dlogprobs = torch.zeros_like(logprobs) \n",
    "dlogprobs[range(n), Yb] = -1.0/n #(Derivate of -logprobs[range(n), Yb].mean() based on above logic)\n",
    "\n",
    "dprobs = (1.0 / probs) * dlogprobs # Derivate of probs.log() (dlog/dx = 1/x) (* dlogprobs because of chain rule) \n",
    "\n",
    "\"\"\"\n",
    "Derivative of counts*counts_sum_inv (counts_sum_inv and counts have different dimensions. So this multiplication takes\n",
    "counts.shape = [32x27], counts_sum_inv.shape = [32x1]\n",
    "place in 2 steps, \n",
    "1) Forecasting (replication) of counts_sum_inv w.r.t every dimension of counts and \n",
    "2) Multiplication\n",
    "So while backpropogating, we will solve for step 2) i.e first find derivative of multiplication i.e.\n",
    "dcounts_sum_inv = counts * dprobs     because,\n",
    "if a = b * c,\n",
    "then da/dc = c\n",
    "therefore dcounts_sum_inv = counts and multiplication with dprobs because chain rule\n",
    "\n",
    "and after this we will solve for step 1). We know in a case when a single tensor gets fed two gradients, we just sum the gradients. \n",
    "Therefore,\n",
    "dcounts_sum_inv = (counts * dprobs).sum(1, keepdims=true) # keepdims true because we want to retain the shape\n",
    "\"\"\"\n",
    "dcounts_sum_inv = (counts * dprobs).sum(1, keepdims=True) \n",
    "dcounts = counts_sum_inv * dprobs\n",
    "dcounts_sum = (-counts_sum**-2) * dcounts_sum_inv # Derivative of counts_sum**-1 (dx**-1/dx = -(x**-2))\n",
    "\"\"\"\n",
    "Gradients divide into equal parts in addition (29:00)\n",
    "+= because dcounts was calculated before as well. So we need to preserve the gradient\n",
    "\"\"\"\n",
    "dcounts += torch.ones_like(counts) * dcounts_sum\n",
    "\n",
    "# Derivative of norm_logits.exp(). (de^x/dx=e^x. As we already calculated norm_logits.exp() in count, we \n",
    "# will use counts instead of norm_logits.exp()\n",
    "dnorm_logits = counts * dcounts \n",
    "\"\"\"\n",
    "Derivative of logits - logit_maxes\n",
    "norm_logits.shape = [32, 27]\n",
    "logits.shape = [32, 27]\n",
    "logit_maxes.shape = [32, 1]\n",
    "So there is a broadcasting happening in minus.\n",
    "Meaning operation is taking place as follows:-\n",
    "c11 c12 c13 = a11 a12 a13   b1\n",
    "c21 c22 c23 = a21 a22 a23 - b2\n",
    "c31 c32 c33 = a31 a32 a33   b3\n",
    "so e.g. c32 = a32 - b3\n",
    "\n",
    "This tells us the local derivative of a[] is 1, e.g. local derivative of a13 = 1 because a13 = (1)a13\n",
    "And the local derivative of b[] is -1, e.g. local derivative of b1 = 1 because b1 = (-1)b1\n",
    "\n",
    "Therefore the derivative of c will flow evenly to a[] and will add it up like before and b[] and we will add it up\n",
    "like before because local derivative of b is -1, it will subtract in case of b.\n",
    "Also, we will calculate dlogits twice because logits has two branches coming out of it in the forward pass.\n",
    "\"\"\"\n",
    "dlogits = dnorm_logits.clone() #Cloning for safety\n",
    "dlogit_maxes = (-dnorm_logits).sum(1, keepdim=True)\n",
    "\n",
    "\"\"\"\n",
    "Derivative of logits.max(1, keepdim=True).values\n",
    "\n",
    "When we perform max() in pytorch, it plucks out the max value of each row of matrix and also tells us what is the index of\n",
    "the plucked out value. These indexes are very important in backward pass because we essentially need to populate values\n",
    "in the matrix at those specific indexes to calculate the gradient.\n",
    "\n",
    "Now local derivate of each plucked out value will be one. Therefore,\n",
    "dlogits = (local derivative that is 1) * dlogit_maxes\n",
    "But because we have gradient in dlogits already from previous operation, we do +=\n",
    "\n",
    "So to scatter the values in matrix at right indexes, we use one hot encoding, where the max value's index will be 1 and\n",
    "rest will be 0. There are other approaches as well (40:18)\n",
    "num_classes=logits.shape[1] means that dimension of each tensor should be logits.shape[1] that is 27\n",
    "\n",
    "Also something to keep in mind is that dlogit_maxes is [32, 1] so when we apply chain rule below, dlogit_maxes will \n",
    "broadcast and be 're-routed' to whichever bits will be 'turned on' in one hot encoded matrix.\n",
    "\n",
    "To visualise run: plt.imshow(F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]))\n",
    "\"\"\"\n",
    "dlogits +=  F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\n",
    "\n",
    "\"\"\"\n",
    "Full explanation (42:00) .... Important\n",
    "Derivative of logits = h @ W2 + b2\n",
    "In matrix multiplication for example if eqn is\n",
    "c = a @ b + c\n",
    "so dl/da = dl/db @ transpose(b) where dl/db = [dl/dd11, dl/dd12]\n",
    "                                              [dl/dd21, dl/dd22]\n",
    "\n",
    "similarly dl/db = transpose(a) @ dl/db\n",
    "and dl/dc = dl/dd * sum(first_dimension)\n",
    "\"\"\"\n",
    "dh = dlogits @ W2.T\n",
    "dW2 = h.T @ dlogits\n",
    "db2 = dlogits.sum(0)\n",
    "\n",
    "\"\"\"\n",
    "derivative of h = torch.tanh(hpreact)\n",
    "if eqn is \n",
    "a = tanh(z) \n",
    "Then da/dz = 1 - a**2\n",
    "\"\"\"\n",
    "dhpreact = ( 1 - h**2) * dh\n",
    "\n",
    "\"\"\"\n",
    "Derivative of hpreact = bngain * bnraw + bnbias\n",
    "hpreact.shape = [32, 64]\n",
    "bngain.shape = [1, 64]\n",
    "bnraw.shape = [32, 64]\n",
    "bnbias.shape = [1, 64]\n",
    "so bngain and bnbias are being broadcasted \n",
    "So,\n",
    "dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "* dhpreact because chain rule and .sum(0, keepdim=True) because all gradients need to sum up vertically because of the shape\n",
    "\n",
    "dbnraw = bngain * dhpreact (chain rule)\n",
    "\n",
    "dbnbias = dhpreact.sum(0, keepdim=true)\n",
    "All the gradients will flow equally to biases and we need to sum them up vertically\n",
    "\n",
    "\"\"\"\n",
    "dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "dbnraw = bngain * dhpreact\n",
    "dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "\n",
    "\"\"\"\n",
    "Derivative of bnraw = bndiff * bnvar_inv\n",
    "bnraw.shape = [32, 64]\n",
    "bndiff.shape = [32, 64]\n",
    "bnvar_inv.shape = [1, 64]\n",
    "So bnvar_inv is being broadcasted\n",
    "\n",
    "We need to keep in mind that bndiff has 2 branches out of it. So this dbndiff is incomplete\n",
    "dbndiff = bnvar_inv * dbnraw\n",
    "\n",
    "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "\"\"\"\n",
    "dbndiff = bnvar_inv * dbnraw\n",
    "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "\n",
    "\"\"\"\n",
    "Derivative of bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "if a = b**2\n",
    "dl/da = 2*b\n",
    "so dbnvar = (-0.5*(bnvar + 1e-5)) * dbnvar_inv\n",
    "\"\"\"\n",
    "dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
    "\n",
    "\"\"\"\n",
    "Derivative of bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True)\n",
    "bnvar.shape = [1, 64]\n",
    "bndiff2.shape = [32, 64]\n",
    "So we see that we are doing the sum over 0th axis of bndiff2 and squashing it down.\n",
    "Therefore there will be some kind of broadcasting in the backward pass.\n",
    "\n",
    "Something to keep in mind is that this behaviour goes hand in hand of squashing in forward pass and broadcasting in \n",
    "backward pass and vice versa.\n",
    "\n",
    "Let a matrix be a11 a12\n",
    "                a21 a22 \n",
    "So summing over columns and scaling will give us b1, b2 where\n",
    "b1 = 1/(n-1)*(a11 + a12)\n",
    "b2 = 1/(n-1)*(a21 + a22)\n",
    "derivative of b1 will be (1/n-1)*(local derivative of a11 and a12) where local derivative of a11 and a12 is 1 and 1. Same goes for b2\n",
    "What this means is derivative of b1 has to flow through a11 and a12, scaled with (1/n-1)\n",
    "So for local derivative, we will create an array of 1s as local derivative is 1\n",
    "\n",
    "Also, when we multiply dbnvar because of chain rule, the calculation takes care of broadcasting on it's own. \n",
    "\"\"\"\n",
    "dbndiff2 = (1/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
    "\n",
    "\"\"\"\n",
    "Derivative of bndiff2 = bndiff**2\n",
    "a = b**3\n",
    "dl/da = 3*b**2\n",
    "we will do += because dbndiff has been calculated before\n",
    "\"\"\"\n",
    "dbndiff += (2*bndiff) * dbndiff2\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Derivative of bndiff = hprebn - bnmeani\n",
    "bndiff.shape = [32, 64]\n",
    "hpredn.shape = [32, 64]\n",
    "bnmeani.shape = [1, 64]\n",
    "So bnmeani is broadcasted in forward pass so there will be sum in backward pass.\n",
    "\n",
    "Because bndiff and hprebn are same shape, the gradient will simply copy so we'll do \n",
    "dhprebn = dbndiff.clone()\n",
    "dbnmeani = (-torch.ones_like(bndiff) * dbndiff).sum(0, keepdims=True)\n",
    "(-) because local derivative is -1\n",
    "Now torch.ones_like(bndiff) * dbndiff won't make much sense so we can just do \n",
    "(-dbndiff).sum(0)\n",
    "\"\"\"\n",
    "dhprebn = dbndiff.clone()\n",
    "dbnmeani = (-dbndiff).sum(0)\n",
    "\n",
    "\"\"\"\n",
    "Derivative of bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "This will be very similar to how we calculated dbndiff2\n",
    "\"\"\"\n",
    "dhprebn += (1.0/n)*torch.ones_like(hprebn) * dbnmeani\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Derivative of hprebn = embcat @ W1 + b1\n",
    "hprebn.shape = [32, 64]\n",
    "embcat.shape = [32, 30]\n",
    "W1.shape = [30, 64]\n",
    "b1.shape = [64]\n",
    "\n",
    "This will be similar to how we calculated dh, dW2 and db2\n",
    "\"\"\"\n",
    "dembcat = dhprebn @ W1.T\n",
    "dW1 = embcat.T @ dhprebn\n",
    "db1 = dhprebn.sum(0)\n",
    "\n",
    "\"\"\"\n",
    "Derivative of embcat = emb.view(emb.shape[0], -1)\n",
    "embcat.shape = [32, 30]\n",
    "emb.shape = [32, 3, 10]\n",
    "\n",
    "So embcat is basically concatenated emb. So we just need to undo the concatenation.\n",
    "\"\"\"\n",
    "demb = dembcat.view(emb.shape)\n",
    "\n",
    "\"\"\"\n",
    "Full explanation (1:22:00)\n",
    "Derivative of emb = C[Xb]\n",
    "emb.shape = [32, 3, 10]\n",
    "C.shape = [27, 10]\n",
    "Xb.shape = [32, 3]\n",
    "\n",
    "With C[Xb], we are just plucking the required row from C and depositing it in emb. So we need to undo this operation.\n",
    "\n",
    "+= because there might be multiple occurences of same character.\n",
    "\"\"\"\n",
    "dC = torch.zeros_like(C)\n",
    "for i in range(Xb.shape[0]):\n",
    "    for j in range(Xb.shape[1]):\n",
    "        ix = Xb[i, j]\n",
    "        dC[ix] += demb[i, j]\n",
    "\n",
    "cmp('logprobs', dlogprobs, logprobs)\n",
    "cmp('probs', dprobs, probs)\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)\n",
    "cmp('counts', dcounts, counts)\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
    "cmp('logits', dlogits, logits)\n",
    "cmp('h', dh, h)\n",
    "cmp('W2', dW2, W2)\n",
    "cmp('b2', db2, b2)\n",
    "cmp('hpreact', dhpreact, hpreact)\n",
    "cmp('bngain', dbngain, bngain)\n",
    "cmp('bnbias', dbnbias, bnbias)\n",
    "cmp('bnraw', dbnraw, bnraw)\n",
    "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "cmp('bnvar', dbnvar, bnvar)\n",
    "cmp('bndiff2', dbndiff2, bndiff2)\n",
    "cmp('bndiff', dbndiff, bndiff)\n",
    "cmp('bnmeani', dbnmeani, bnmeani)\n",
    "cmp('hprebn', dhprebn, hprebn)\n",
    "cmp('embcat', dembcat, embcat)\n",
    "cmp('W1', dW1, W1)\n",
    "cmp('b1', db1, b1)\n",
    "cmp('emb', demb, emb)\n",
    "cmp('C', dC, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49bc3a7a-af65-4b0f-9585-330d7b1d6e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.339176654815674 diff: 2.384185791015625e-07\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2: backprop through cross_entropy but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the loss,\n",
    "# take the derivative, simplify the expression, and just write it out\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# logit_maxes = logits.max(1, keepdim=True).values\n",
    "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "# counts = norm_logits.exp()\n",
    "# counts_sum = counts.sum(1, keepdims=True)\n",
    "# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "# probs = counts * counts_sum_inv\n",
    "# logprobs = probs.log()\n",
    "# loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# now:\n",
    "loss_fast = F.cross_entropy(logits, Yb)\n",
    "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8ade405-1542-4cbf-b25c-ee4cf201feab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits          | exact: False | approximate: True  | maxdiff: 6.984919309616089e-09\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "\"\"\"\n",
    "Full explanation (1:26:38)\n",
    "\"\"\"\n",
    "# -----------------\n",
    "# YOUR CODE HERE :)\n",
    "dlogits = F.softmax(logits, 1)\n",
    "dlogits[range(n), Yb] -= 1\n",
    "dlogits /= n\n",
    "# -----------------\n",
    "\n",
    "cmp('logits', dlogits, logits) # I can only get approximate to be true, my maxdiff is 6e-9"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
