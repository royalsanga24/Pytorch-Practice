{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c857f5ef-9847-42bc-9b14-f16c99c011fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a5af913-949d-4c81-892b-995cca2dc4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n",
      "15\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
     ]
    }
   ],
   "source": [
    "# read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "print(len(words))\n",
    "print(max(len(w) for w in words))\n",
    "print(words[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "193ac9ca-9a1d-45b1-a5ea-7f72c63ccbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "447b4c90-3e05-41f9-a789-c6ff4263e023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec022e9b-379a-46fe-b9aa-f7fbc821ac11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
    "def cmp(s, dt, t):\n",
    "  ex = torch.all(dt == t.grad).item()\n",
    "  app = torch.allclose(dt, t.grad)\n",
    "  maxdiff = (dt - t.grad).abs().max().item()\n",
    "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b717c58-9519-44cf-a87f-7795ce6aceaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "# Note: I am initializating many of these parameters in non-standard ways\n",
    "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
    "# implementation of the backward pass.\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77ca85dc-cfd6-43de-b3f4-9b870bdf44ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size # a shorter variable also, for convenience\n",
    "# construct a minibatch\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dd31d4d-94da-4577-92da-912b114ae812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3382, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
    "\n",
    "emb = C[Xb] # embed the characters into vectors\n",
    "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "# Linear layer 1\n",
    "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "# BatchNorm layer\n",
    "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "# Non-linearity\n",
    "h = torch.tanh(hpreact) # hidden layer\n",
    "# Linear layer 2\n",
    "logits = h @ W2 + b2 # output layer\n",
    "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdims=True)\n",
    "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# PyTorch backward pass\n",
    "for p in parameters:\n",
    "  p.grad = None\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
    "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
    "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
    "         embcat, emb]:\n",
    "  t.retain_grad()\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "639e25a1-2263-4740-974f-2be84c27706c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]),\n",
       " torch.Size([32, 1]),\n",
       " torch.Size([32, 1]),\n",
       " torch.Size([32, 27]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.shape, counts_sum.shape, counts_sum_inv.shape, probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1fb6f398-4f62-4e55-9a81-b80d3f12ec91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hpreact         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bngain          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnbias          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnraw           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar_inv       | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar           | exact: False | approximate: False | maxdiff: 0.12878336012363434\n",
      "bndiff          | exact: False | approximate: False | maxdiff: 0.0010164049454033375\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: backprop through the whole thing manually, \n",
    "# backpropagating through exactly all of the variables \n",
    "# as they are defined in the forward pass above, one by one\n",
    "\n",
    "'''\n",
    "for example mean of a, b, c is\n",
    "mean = (a + b + c) / 3 (NLL)\n",
    "loss = -(a + b + c) / 3\n",
    "loss = -1/3a + -1/3b + -1/3c\n",
    "dloss/da = -1/3\n",
    "dloss/db = -1/3\n",
    "dloss/dc = -1/3\n",
    "\n",
    "Therefore, generally dloss/da = -1/n\n",
    "'''\n",
    "'''\n",
    "Additionaly we can see that logprobs is [32x27] and Yb is [32].\n",
    "But in loss we are not taking account of other values of logprobs and shrinking it to Yb i.e 32. So what happens to gradient of other \n",
    "values of logprobs?\n",
    "Well they don't matter as they don't contribute to the loss.\n",
    "'''\n",
    "dlogprobs = torch.zeros_like(logprobs) \n",
    "dlogprobs[range(n), Yb] = -1.0/n #(Derivate of -logprobs[range(n), Yb].mean() based on above logic)\n",
    "\n",
    "dprobs = (1.0 / probs) * dlogprobs # Derivate of probs.log() (dlog/dx = 1/x) (* dlogprobs because of chain rule) \n",
    "\n",
    "\"\"\"\n",
    "Derivative of counts*counts_sum_inv (counts_sum_inv and counts have different dimensions. So this multiplication takes\n",
    "counts.shape = [32x27], counts_sum_inv.shape = [32x1]\n",
    "place in 2 steps, \n",
    "1) Forecasting (replication) of counts_sum_inv w.r.t every dimension of counts and \n",
    "2) Multiplication\n",
    "So while backpropogating, we will solve for step 2) i.e first find derivative of multiplication i.e.\n",
    "dcounts_sum_inv = counts * dprobs     because,\n",
    "if a = b * c,\n",
    "then da/dc = c\n",
    "therefore dcounts_sum_inv = counts and multiplication with dprobs because chain rule\n",
    "\n",
    "and after this we will solve for step 1). We know in a case when a single tensor gets fed two gradients, we just sum the gradients. \n",
    "Therefore,\n",
    "dcounts_sum_inv = (counts * dprobs).sum(1, keepdims=true) # keepdims true because we want to retain the shape\n",
    "\"\"\"\n",
    "dcounts_sum_inv = (counts * dprobs).sum(1, keepdims=True) \n",
    "dcounts = counts_sum_inv * dprobs\n",
    "dcounts_sum = (-counts_sum**-2) * dcounts_sum_inv # Derivative of counts_sum**-1 (dx**-1/dx = -(x**-2))\n",
    "\"\"\"\n",
    "Gradients divide into equal parts in addition (29:00)\n",
    "+= because dcounts was calculated before as well. So we need to preserve the gradient\n",
    "\"\"\"\n",
    "dcounts += torch.ones_like(counts) * dcounts_sum\n",
    "\n",
    "# Derivative of norm_logits.exp(). (de^x/dx=e^x. As we already calculated norm_logits.exp() in count, we \n",
    "# will use counts instead of norm_logits.exp()\n",
    "dnorm_logits = counts * dcounts \n",
    "\"\"\"\n",
    "Derivative of logits - logit_maxes\n",
    "norm_logits.shape = [32, 27]\n",
    "logits.shape = [32, 27]\n",
    "logit_maxes.shape = [32, 1]\n",
    "So there is a broadcasting happening in minus.\n",
    "Meaning operation is taking place as follows:-\n",
    "c11 c12 c13 = a11 a12 a13   b1\n",
    "c21 c22 c23 = a21 a22 a23 - b2\n",
    "c31 c32 c33 = a31 a32 a33   b3\n",
    "so e.g. c32 = a32 - b3\n",
    "\n",
    "This tells us the local derivative of a[] is 1, e.g. local derivative of a13 = 1 because a13 = (1)a13\n",
    "And the local derivative of b[] is -1, e.g. local derivative of b1 = 1 because b1 = (-1)b1\n",
    "\n",
    "Therefore the derivative of c will flow evenly to a[] and will add it up like before and b[] and we will add it up\n",
    "like before because local derivative of b is -1, it will subtract in case of b.\n",
    "Also, we will calculate dlogits twice because logits has two branches coming out of it in the forward pass.\n",
    "\"\"\"\n",
    "dlogits = dnorm_logits.clone() #Cloning for safety\n",
    "dlogit_maxes = (-dnorm_logits).sum(1, keepdim=True)\n",
    "\n",
    "\"\"\"\n",
    "Derivative of logits.max(1, keepdim=True).values\n",
    "\n",
    "When we perform max() in pytorch, it plucks out the max value of each row of matrix and also tells us what is the index of\n",
    "the plucked out value. These indexes are very important in backward pass because we essentially need to populate values\n",
    "in the matrix at those specific indexes to calculate the gradient.\n",
    "\n",
    "Now local derivate of each plucked out value will be one. Therefore,\n",
    "dlogits = (local derivative that is 1) * dlogit_maxes\n",
    "But because we have gradient in dlogits already from previous operation, we do +=\n",
    "\n",
    "So to scatter the values in matrix at right indexes, we use one hot encoding, where the max value's index will be 1 and\n",
    "rest will be 0. There are other approaches as well (40:18)\n",
    "num_classes=logits.shape[1] means that dimension of each tensor should be logits.shape[1] that is 27\n",
    "\n",
    "Also something to keep in mind is that dlogit_maxes is [32, 1] so when we apply chain rule below, dlogit_maxes will \n",
    "broadcast and be 're-routed' to whichever bits will be 'turned on' in one hot encoded matrix.\n",
    "\n",
    "To visualise run: plt.imshow(F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]))\n",
    "\"\"\"\n",
    "dlogits +=  F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\n",
    "\n",
    "\"\"\"\n",
    "Full explanation (42:00) .... Important\n",
    "Derivative of logits = h @ W2 + b2\n",
    "In matrix multiplication for example if eqn is\n",
    "c = a @ b + c\n",
    "so dl/da = dl/db @ transpose(b) where dl/db = [dl/dd11, dl/dd12]\n",
    "                                              [dl/dd21, dl/dd22]\n",
    "\n",
    "similarly dl/db = transpose(a) @ dl/db\n",
    "and dl/dc = dl/dd * sum(first_dimension)\n",
    "\"\"\"\n",
    "dh = dlogits @ W2.T\n",
    "dW2 = h.T @ dlogits\n",
    "db2 = dlogits.sum(0)\n",
    "\n",
    "\"\"\"\n",
    "derivative of h = torch.tanh(hpreact)\n",
    "if eqn is \n",
    "a = tanh(z) \n",
    "Then da/dz = 1 - a**2\n",
    "\"\"\"\n",
    "dhpreact = ( 1 - h**2) * dh\n",
    "\n",
    "\"\"\"\n",
    "Derivative of hpreact = bngain * bnraw + bnbias\n",
    "hpreact.shape = [32, 64]\n",
    "bngain.shape = [1, 64]\n",
    "bnraw.shape = [32, 64]\n",
    "bnbias.shape = [1, 64]\n",
    "so bngain and bnbias are being broadcasted \n",
    "So,\n",
    "dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "* dhpreact because chain rule and .sum(0, keepdim=True) because all gradients need to sum up vertically because of the shape\n",
    "\n",
    "dbnraw = bngain * dhpreact (chain rule)\n",
    "\n",
    "dbnbias = dhpreact.sum(0, keepdim=true)\n",
    "All the gradients will flow equally to biases and we need to sum them up vertically\n",
    "\n",
    "\"\"\"\n",
    "dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "dbnraw = bngain * dhpreact\n",
    "dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "\n",
    "\"\"\"\n",
    "Derivative of bnraw = bndiff * bnvar_inv\n",
    "bnraw.shape = [32, 64]\n",
    "bndiff.shape = [32, 64]\n",
    "bnvar_inv.shape = [1, 64]\n",
    "So bnvar_inv is being broadcasted\n",
    "\n",
    "We need to keep in mind that bndiff has 2 branches out of it. So this dbndiff is incomplete\n",
    "dbndiff = bnvar_inv * dbnraw\n",
    "\n",
    "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "\"\"\"\n",
    "dbndiff = bnvar_inv * dbnraw\n",
    "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "\n",
    "\"\"\"\n",
    "Derivative of bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "if a = b**2\n",
    "dl/da = 2*b\n",
    "so dbnvar = (-0.5*(bnvar + 1e-5)) * dbnvar_inv\n",
    "\"\"\"\n",
    "dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
    "\n",
    "cmp('logprobs', dlogprobs, logprobs)\n",
    "cmp('probs', dprobs, probs)\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)\n",
    "cmp('counts', dcounts, counts)\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
    "cmp('logits', dlogits, logits)\n",
    "cmp('h', dh, h)\n",
    "cmp('W2', dW2, W2)\n",
    "cmp('b2', db2, b2)\n",
    "cmp('hpreact', dhpreact, hpreact)\n",
    "cmp('bngain', dbngain, bngain)\n",
    "cmp('bnbias', dbnbias, bnbias)\n",
    "cmp('bnraw', dbnraw, bnraw)\n",
    "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "cmp('bnvar', dbnvar, bnvar)\n",
    "# cmp('bndiff2', dbndiff2, bndiff2)\n",
    "cmp('bndiff', dbndiff, bndiff)\n",
    "# cmp('bnmeani', dbnmeani, bnmeani)\n",
    "# cmp('hprebn', dhprebn, hprebn)\n",
    "# cmp('embcat', dembcat, embcat)\n",
    "# cmp('W1', dW1, W1)\n",
    "# cmp('b1', db1, b1)\n",
    "# cmp('emb', demb, emb)\n",
    "# cmp('C', dC, C)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
