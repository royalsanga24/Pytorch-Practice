{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad992ee5-9bb2-43d8-9b25-2688878e43bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "557ad851-563e-47b8-9b61-06193c202a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open(\"names.txt\", \"r\").read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b79a29d4-309c-4f87-aae5-5b07e4e8c1eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd864135-73e1-433c-92c8-e8200d91b8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s: i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12a6a4f1-026b-4f10-91f2-d3f9b04e55aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  0,  0],\n",
       "         [ 0,  0,  5],\n",
       "         [ 0,  5, 13],\n",
       "         [ 5, 13, 13],\n",
       "         [13, 13,  1],\n",
       "         [ 0,  0,  0],\n",
       "         [ 0,  0, 15],\n",
       "         [ 0, 15, 12],\n",
       "         [15, 12,  9],\n",
       "         [12,  9, 22],\n",
       "         [ 9, 22,  9],\n",
       "         [22,  9,  1],\n",
       "         [ 0,  0,  0],\n",
       "         [ 0,  0,  1],\n",
       "         [ 0,  1, 22],\n",
       "         [ 1, 22,  1],\n",
       "         [ 0,  0,  0],\n",
       "         [ 0,  0,  9],\n",
       "         [ 0,  9, 19],\n",
       "         [ 9, 19,  1],\n",
       "         [19,  1,  2],\n",
       "         [ 1,  2,  5],\n",
       "         [ 2,  5, 12],\n",
       "         [ 5, 12, 12],\n",
       "         [12, 12,  1],\n",
       "         [ 0,  0,  0],\n",
       "         [ 0,  0, 19],\n",
       "         [ 0, 19, 15],\n",
       "         [19, 15, 16],\n",
       "         [15, 16,  8],\n",
       "         [16,  8,  9],\n",
       "         [ 8,  9,  1]]),\n",
       " tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0,  1, 22,  1,  0,  9, 19,\n",
       "          1,  2,  5, 12, 12,  1,  0, 19, 15, 16,  8,  9,  1,  0]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 3\n",
    "X, Y = [], []\n",
    "for w in words[:5]:\n",
    "    context = [0]*block_size\n",
    "    for ch in w+'.':\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        context = context[1:] + [ix]\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4b0ea4f-21d0-435f-afc8-350dd24d5c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.Size([32]), torch.int64, torch.int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape, X.dtype, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1df8557-b609-4fb5-8f19-dacb72e79ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.randn((27,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "063701d5-b823-4085-b6f8-de9185878e98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0930, -0.4268],\n",
       "        [ 0.0820, -0.0620],\n",
       "        [-0.2449, -0.0207],\n",
       "        [ 0.4452, -0.4018],\n",
       "        [ 0.5828,  0.3679],\n",
       "        [-0.3277,  1.2833],\n",
       "        [-0.5567, -1.9257],\n",
       "        [-1.0785, -1.1523],\n",
       "        [ 1.3762, -0.1978],\n",
       "        [ 1.0277,  1.3164],\n",
       "        [-1.1770, -0.7530],\n",
       "        [ 0.1240, -0.2351],\n",
       "        [-1.6998, -0.3647],\n",
       "        [ 1.5099,  0.1672],\n",
       "        [-1.8065,  0.0753],\n",
       "        [ 0.3375, -1.0636],\n",
       "        [-0.0305,  0.5801],\n",
       "        [-0.8858,  0.1821],\n",
       "        [ 1.1037,  0.2540],\n",
       "        [-0.3678, -1.2620],\n",
       "        [-0.0099,  0.5427],\n",
       "        [ 0.8055, -0.3053],\n",
       "        [ 1.9682, -1.6132],\n",
       "        [-2.0108,  0.5969],\n",
       "        [ 0.6811,  1.0935],\n",
       "        [ 0.4655,  0.4064],\n",
       "        [ 0.8871,  0.3550]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "671de702-25e9-4bf9-9ac4-d11ee4889b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3, 2]),\n",
       " tensor([[[-1.0930, -0.4268],\n",
       "          [-1.0930, -0.4268],\n",
       "          [-1.0930, -0.4268]],\n",
       " \n",
       "         [[-1.0930, -0.4268],\n",
       "          [-1.0930, -0.4268],\n",
       "          [-0.3277,  1.2833]],\n",
       " \n",
       "         [[-1.0930, -0.4268],\n",
       "          [-0.3277,  1.2833],\n",
       "          [ 1.5099,  0.1672]],\n",
       " \n",
       "         [[-0.3277,  1.2833],\n",
       "          [ 1.5099,  0.1672],\n",
       "          [ 1.5099,  0.1672]],\n",
       " \n",
       "         [[ 1.5099,  0.1672],\n",
       "          [ 1.5099,  0.1672],\n",
       "          [ 0.0820, -0.0620]],\n",
       " \n",
       "         [[-1.0930, -0.4268],\n",
       "          [-1.0930, -0.4268],\n",
       "          [-1.0930, -0.4268]],\n",
       " \n",
       "         [[-1.0930, -0.4268],\n",
       "          [-1.0930, -0.4268],\n",
       "          [ 0.3375, -1.0636]],\n",
       " \n",
       "         [[-1.0930, -0.4268],\n",
       "          [ 0.3375, -1.0636],\n",
       "          [-1.6998, -0.3647]],\n",
       " \n",
       "         [[ 0.3375, -1.0636],\n",
       "          [-1.6998, -0.3647],\n",
       "          [ 1.0277,  1.3164]],\n",
       " \n",
       "         [[-1.6998, -0.3647],\n",
       "          [ 1.0277,  1.3164],\n",
       "          [ 1.9682, -1.6132]],\n",
       " \n",
       "         [[ 1.0277,  1.3164],\n",
       "          [ 1.9682, -1.6132],\n",
       "          [ 1.0277,  1.3164]],\n",
       " \n",
       "         [[ 1.9682, -1.6132],\n",
       "          [ 1.0277,  1.3164],\n",
       "          [ 0.0820, -0.0620]],\n",
       " \n",
       "         [[-1.0930, -0.4268],\n",
       "          [-1.0930, -0.4268],\n",
       "          [-1.0930, -0.4268]],\n",
       " \n",
       "         [[-1.0930, -0.4268],\n",
       "          [-1.0930, -0.4268],\n",
       "          [ 0.0820, -0.0620]],\n",
       " \n",
       "         [[-1.0930, -0.4268],\n",
       "          [ 0.0820, -0.0620],\n",
       "          [ 1.9682, -1.6132]],\n",
       " \n",
       "         [[ 0.0820, -0.0620],\n",
       "          [ 1.9682, -1.6132],\n",
       "          [ 0.0820, -0.0620]],\n",
       " \n",
       "         [[-1.0930, -0.4268],\n",
       "          [-1.0930, -0.4268],\n",
       "          [-1.0930, -0.4268]],\n",
       " \n",
       "         [[-1.0930, -0.4268],\n",
       "          [-1.0930, -0.4268],\n",
       "          [ 1.0277,  1.3164]],\n",
       " \n",
       "         [[-1.0930, -0.4268],\n",
       "          [ 1.0277,  1.3164],\n",
       "          [-0.3678, -1.2620]],\n",
       " \n",
       "         [[ 1.0277,  1.3164],\n",
       "          [-0.3678, -1.2620],\n",
       "          [ 0.0820, -0.0620]],\n",
       " \n",
       "         [[-0.3678, -1.2620],\n",
       "          [ 0.0820, -0.0620],\n",
       "          [-0.2449, -0.0207]],\n",
       " \n",
       "         [[ 0.0820, -0.0620],\n",
       "          [-0.2449, -0.0207],\n",
       "          [-0.3277,  1.2833]],\n",
       " \n",
       "         [[-0.2449, -0.0207],\n",
       "          [-0.3277,  1.2833],\n",
       "          [-1.6998, -0.3647]],\n",
       " \n",
       "         [[-0.3277,  1.2833],\n",
       "          [-1.6998, -0.3647],\n",
       "          [-1.6998, -0.3647]],\n",
       " \n",
       "         [[-1.6998, -0.3647],\n",
       "          [-1.6998, -0.3647],\n",
       "          [ 0.0820, -0.0620]],\n",
       " \n",
       "         [[-1.0930, -0.4268],\n",
       "          [-1.0930, -0.4268],\n",
       "          [-1.0930, -0.4268]],\n",
       " \n",
       "         [[-1.0930, -0.4268],\n",
       "          [-1.0930, -0.4268],\n",
       "          [-0.3678, -1.2620]],\n",
       " \n",
       "         [[-1.0930, -0.4268],\n",
       "          [-0.3678, -1.2620],\n",
       "          [ 0.3375, -1.0636]],\n",
       " \n",
       "         [[-0.3678, -1.2620],\n",
       "          [ 0.3375, -1.0636],\n",
       "          [-0.0305,  0.5801]],\n",
       " \n",
       "         [[ 0.3375, -1.0636],\n",
       "          [-0.0305,  0.5801],\n",
       "          [ 1.3762, -0.1978]],\n",
       " \n",
       "         [[-0.0305,  0.5801],\n",
       "          [ 1.3762, -0.1978],\n",
       "          [ 1.0277,  1.3164]],\n",
       " \n",
       "         [[ 1.3762, -0.1978],\n",
       "          [ 1.0277,  1.3164],\n",
       "          [ 0.0820, -0.0620]]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create embedding matrix\n",
    "emb = C[X]\n",
    "emb.shape, emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bbf671f-e065-4b3b-b5a0-8d609d6b601c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 0, 1]),\n",
       " tensor([[-1.0930, -0.4268],\n",
       "         [-1.0930, -0.4268],\n",
       "         [ 0.0820, -0.0620]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[13], C[X][13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec3a2b27-1646-47b7-8c28-8ed2029f358b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-9.2211e-01,  7.5306e-01,  7.8995e-01, -7.2484e-01, -3.9582e-01,\n",
       "          -7.2822e-01, -9.3285e-01,  8.0370e-01, -6.2950e-01, -1.2483e+00,\n",
       "           1.3236e-01,  2.7504e-01,  1.4655e-02,  4.1024e-01, -3.2124e-01,\n",
       "          -6.5802e-01,  3.7295e-01,  1.0444e+00, -4.7792e-01,  1.0373e+00,\n",
       "           4.7160e-01, -1.4727e+00, -2.6258e-01,  6.1874e-01, -2.5905e-03,\n",
       "          -5.0684e-01, -2.6885e-01,  9.1046e-01,  5.0543e-01,  3.7209e-01,\n",
       "          -1.0848e+00,  9.7717e-01,  1.4167e+00,  5.6252e-01, -1.3684e-01,\n",
       "          -1.3115e+00, -5.3103e-01, -1.9792e+00, -5.5579e-01,  4.3327e-01,\n",
       "          -1.0561e+00,  2.6706e-01, -1.6396e+00,  1.4795e+00,  1.0566e+00,\n",
       "          -7.1630e-01, -7.0903e-01, -1.3207e+00,  7.3630e-01,  1.4228e+00,\n",
       "          -4.1350e-01,  1.8248e+00, -4.6528e-01,  1.0478e+00,  6.3121e-01,\n",
       "          -6.2098e-01, -4.1941e-01, -5.5113e-01, -4.5087e-01,  8.8973e-01,\n",
       "           1.4701e+00, -7.4405e-01, -8.7177e-01, -3.7834e-01,  3.3891e-01,\n",
       "           9.0769e-01, -1.0723e+00,  2.5659e-01, -9.4477e-01, -4.5348e-01,\n",
       "           3.9942e-01,  5.4021e-01,  1.3332e-01, -1.4645e+00,  5.3741e-01,\n",
       "          -2.9443e-01,  3.4431e-01, -7.1153e-01, -1.6322e-01, -9.9168e-01,\n",
       "          -1.6688e-01, -2.6792e-01,  2.8621e-01,  9.1758e-01,  3.1797e+00,\n",
       "           1.3144e+00,  7.6287e-01,  1.0839e+00, -1.6106e+00, -1.0152e+00,\n",
       "          -2.6397e-01, -1.1398e+00,  3.9447e-01,  1.3754e+00,  1.8429e+00,\n",
       "          -2.2361e-01, -1.0639e+00, -9.8255e-02,  1.2398e-01,  1.4037e+00],\n",
       "         [-1.9295e-01,  7.5699e-01, -2.6462e-01,  4.7863e-01, -8.9050e-01,\n",
       "           1.6505e-01, -1.6091e+00, -2.2390e+00, -2.0070e+00, -6.3096e-01,\n",
       "           2.1226e-01, -6.0432e-01, -6.8307e-02,  1.2150e+00, -6.5923e-01,\n",
       "          -4.1744e-01, -9.1668e-02,  8.5998e-01,  1.7427e+00, -1.4774e+00,\n",
       "          -5.3323e-01,  2.2958e-01, -1.0392e-02, -1.1199e+00,  1.4132e-01,\n",
       "           1.5154e-01,  9.4922e-01,  9.2605e-01, -1.8501e+00, -7.9156e-01,\n",
       "          -4.6012e-01,  3.7570e-01,  4.6220e-01,  7.1618e-01, -2.2166e-01,\n",
       "          -8.6203e-01,  7.8411e-02,  9.8837e-02, -1.0135e+00,  1.1395e+00,\n",
       "          -1.8042e+00,  5.1941e-01,  4.4900e-01, -8.8546e-01,  1.9163e-01,\n",
       "           8.6237e-01,  2.2644e-01,  1.4628e+00, -9.7132e-01,  7.3561e-01,\n",
       "           1.7830e+00,  7.5986e-01,  1.5584e+00, -6.1753e-01, -3.6224e-01,\n",
       "          -5.5175e-01,  6.2730e-01, -1.0928e+00, -6.4399e-01,  1.5761e+00,\n",
       "          -1.1261e+00, -3.1852e-01,  4.3292e-01,  1.0533e+00, -9.4912e-01,\n",
       "           2.4377e-01,  3.9942e-01, -4.6067e-01, -1.4072e+00, -2.6757e+00,\n",
       "          -2.4741e-01,  3.3334e-01, -8.9752e-01,  4.1750e-01, -1.2443e-01,\n",
       "          -2.5208e-01, -6.7657e-01,  2.4205e-01,  1.0987e+00, -1.1968e+00,\n",
       "           1.7885e-01, -2.0244e-01,  1.1601e+00, -7.8719e-01,  8.3080e-02,\n",
       "           3.2975e-01, -8.6754e-01, -7.1623e-01, -9.3471e-02, -8.1157e-02,\n",
       "           7.7549e-01, -1.4340e-01,  2.1266e-01,  6.9527e-01,  7.4694e-01,\n",
       "           6.7442e-02,  4.6480e-01, -4.8589e-02,  7.2540e-01, -1.7573e+00],\n",
       "         [ 1.9083e+00, -2.5913e-01,  4.5084e-01,  5.0676e-01, -9.8339e-01,\n",
       "          -2.8855e-01, -8.2395e-01, -6.3936e-01,  3.6628e-01,  1.9561e-01,\n",
       "          -1.0020e-01, -7.2905e-01, -8.8359e-01, -7.3767e-01, -2.4857e-01,\n",
       "           1.4847e+00, -5.4560e-01,  5.6592e-01,  1.3742e+00,  8.2736e-01,\n",
       "           1.2952e+00,  5.3716e-02,  6.3052e-01, -7.1356e-01, -6.2032e-01,\n",
       "          -7.7594e-02, -6.4901e-01, -1.6481e+00,  5.5849e-01, -3.5048e-01,\n",
       "           4.3853e-01, -4.3117e-01, -7.6804e-02,  7.7704e-01,  8.3366e-02,\n",
       "          -4.7088e-01, -8.3589e-01,  1.1165e+00,  1.4062e+00, -1.5279e+00,\n",
       "          -5.7535e-02,  1.2605e+00, -6.1717e-01,  1.3904e+00, -6.2716e-01,\n",
       "          -2.8885e-01,  9.6387e-01, -1.7052e+00,  1.9234e+00, -2.1540e-01,\n",
       "          -7.3013e-01, -2.0916e+00, -6.5827e-01,  3.1251e+00,  1.5338e+00,\n",
       "           4.2399e-02,  1.1860e-01, -1.9496e-01, -2.3654e+00,  1.4265e+00,\n",
       "          -1.0211e+00, -3.4440e-01, -6.6751e-01,  9.7353e-01,  1.1431e-01,\n",
       "          -8.3004e-01, -9.7608e-02,  7.2909e-01, -6.0951e-01,  6.5047e-01,\n",
       "           9.7406e-01,  1.4089e+00,  3.8869e-01, -2.0523e-01,  1.6821e-02,\n",
       "          -1.5940e+00,  2.0676e-01,  1.4448e+00,  1.2520e+00,  6.5904e-01,\n",
       "           2.8698e-01,  2.1936e-01, -1.0759e+00,  5.9862e-01, -3.4943e-01,\n",
       "          -5.3394e-01, -8.5928e-01, -1.3584e-01, -4.6720e-01, -1.0713e+00,\n",
       "           1.9333e+00,  5.0206e-01, -2.0159e+00, -1.2385e+00, -8.1310e-01,\n",
       "          -3.1303e-01, -1.9474e+00,  1.5044e+00,  6.1905e-01, -1.2515e+00],\n",
       "         [-8.7955e-02, -3.0456e-01,  1.6678e-01, -1.2695e+00,  8.2382e-01,\n",
       "           9.5260e-01, -5.5943e-01,  1.1376e+00,  7.2010e-01,  5.3727e-01,\n",
       "           1.4160e+00,  6.9808e-01,  1.3036e+00,  1.9807e+00,  2.0549e+00,\n",
       "          -1.4599e+00,  1.2334e+00,  4.3866e-01,  5.7835e-01,  1.3326e+00,\n",
       "           7.6997e-01,  7.3942e-01,  1.6118e-01,  1.2875e-01,  9.7421e-01,\n",
       "          -8.8955e-01,  1.5958e+00, -4.4057e-01,  1.6778e+00, -1.7382e+00,\n",
       "           4.8583e-01,  9.3870e-02, -5.8004e-02,  2.7081e-01, -1.0207e+00,\n",
       "          -3.6195e-02,  1.0421e+00, -1.4062e+00,  3.1189e-01, -1.7127e+00,\n",
       "          -6.6197e-01,  1.0577e+00,  1.4900e+00, -1.1917e+00, -1.6068e+00,\n",
       "           1.2303e+00,  2.5633e-01,  5.2774e-01, -1.0367e-01,  1.0752e+00,\n",
       "          -1.2612e+00, -1.3808e+00, -1.3546e+00,  4.2763e-02, -1.1328e+00,\n",
       "          -1.7039e+00,  5.7246e-01,  8.7301e-01, -3.8501e-01,  9.2408e-01,\n",
       "          -2.7196e+00,  8.0896e-01, -1.5471e-01,  1.1307e+00,  5.9601e-01,\n",
       "           2.2164e-01, -1.0327e+00,  1.9478e+00, -2.0161e+00,  4.1271e-01,\n",
       "           4.1933e-01,  1.6476e-01,  5.2167e-01,  9.9530e-01, -1.2683e+00,\n",
       "          -4.2276e-02,  3.9896e-01, -9.5383e-02,  4.7938e-02, -2.0251e+00,\n",
       "          -5.4919e-01, -1.8009e+00, -1.2229e-01, -1.5484e+00, -2.6562e-01,\n",
       "           4.9320e-01,  6.8491e-01,  3.9029e-01, -7.5843e-01,  3.7018e-01,\n",
       "           4.3100e-01, -1.9167e+00,  1.1485e+00, -2.1645e+00,  1.2557e+00,\n",
       "          -9.7189e-01,  1.1773e+00,  1.4465e-01, -9.6955e-01,  6.9204e-01],\n",
       "         [-1.1140e+00, -1.0470e+00,  4.5686e-01,  4.0390e-01,  1.0020e-01,\n",
       "           9.8421e-01,  4.8236e-03,  1.5298e-01, -6.4313e-01, -1.1194e+00,\n",
       "           1.6837e+00,  1.3488e+00,  4.2273e-01,  2.7387e-01, -1.0518e+00,\n",
       "          -1.0297e+00, -9.6924e-01, -2.1494e-01, -1.2498e+00, -1.0909e+00,\n",
       "          -9.9236e-01, -1.1420e+00,  1.9189e+00,  2.3799e-01,  2.8612e+00,\n",
       "          -1.3369e-01, -1.1124e+00,  4.9057e-01,  2.9457e-01,  9.7565e-01,\n",
       "           5.0548e-01, -2.9456e-01, -1.1256e+00,  1.8808e-01, -8.3023e-01,\n",
       "          -2.1717e+00,  6.7977e-01,  6.0678e-01,  5.1729e-02,  7.0864e-01,\n",
       "          -8.9929e-01, -1.1431e+00, -9.8516e-01,  3.9779e-01, -4.2425e-01,\n",
       "           3.6030e-02, -3.4661e-02, -1.2051e+00,  1.5418e+00,  4.5474e-01,\n",
       "          -3.3570e+00,  9.2883e-01, -1.2968e-01,  3.7584e-02, -7.4201e-01,\n",
       "           7.4412e-01,  7.6418e-01, -2.1623e+00, -4.0153e-01,  1.0593e+00,\n",
       "           4.0083e-01,  7.2191e-01,  8.9771e-01,  1.0405e+00,  1.3320e+00,\n",
       "           9.4639e-02,  1.1546e+00,  2.8976e-01, -1.5890e+00, -9.9936e-01,\n",
       "           5.4881e-01,  6.9356e-01, -9.9516e-01,  3.4226e-01,  1.0998e+00,\n",
       "          -5.6589e-01,  7.0978e-01, -1.7652e+00,  1.8743e+00, -8.4046e-01,\n",
       "          -8.1708e-01,  1.6691e+00,  8.5392e-01,  4.5743e-01,  2.4774e-01,\n",
       "           1.3297e+00,  3.0277e+00,  1.9579e-01,  1.2252e+00, -3.1637e-01,\n",
       "           1.1751e+00, -5.3403e-02, -6.0091e-02, -4.8095e-01,  2.6393e+00,\n",
       "           1.6327e-01,  3.3611e-01,  2.2360e+00, -9.3908e-01,  1.0353e+00],\n",
       "         [-6.1742e-01,  2.1441e-01,  3.0999e-01,  8.5427e-01, -2.3880e-01,\n",
       "          -9.3725e-01, -2.0965e-02,  2.7917e-01,  1.6835e-01, -8.6368e-01,\n",
       "           1.2350e+00,  2.2036e+00,  3.7604e-01, -1.9298e+00,  1.6959e+00,\n",
       "          -5.8489e-01,  1.1833e+00,  2.6045e-02,  1.1261e-01, -8.3160e-01,\n",
       "          -5.8824e-01,  1.1566e+00,  1.2772e+00,  4.9012e-01, -7.4807e-01,\n",
       "           1.4886e+00, -1.1044e+00,  9.6313e-01,  1.6001e+00, -1.3996e+00,\n",
       "           9.6872e-01,  2.1471e+00, -9.5162e-01,  6.5419e-01, -1.3043e-01,\n",
       "           6.9342e-01,  2.0649e-01,  1.1669e+00,  8.9073e-01, -1.6238e+00,\n",
       "           2.6608e+00,  9.9310e-01,  2.0514e-01,  1.2624e-01, -1.0397e+00,\n",
       "           1.8514e+00,  3.2984e-01,  2.7460e-01, -6.6775e-01,  2.1932e-01,\n",
       "          -4.4840e-01, -1.9639e+00,  5.6638e-01, -4.6244e-01, -2.0303e+00,\n",
       "           5.5794e-01, -2.6105e+00, -9.8981e-01, -9.0385e-01, -1.4238e-01,\n",
       "           1.5057e-02,  7.2216e-01, -1.6213e+00,  5.8375e-02, -4.6718e-01,\n",
       "           1.0332e+00,  6.5639e-01,  6.5374e-01, -3.6435e-01, -5.8553e-01,\n",
       "          -1.0604e+00, -6.9021e-01, -2.2678e+00, -6.1860e-01, -1.0194e+00,\n",
       "          -4.8407e-01, -1.8312e+00, -8.8535e-01, -7.0470e-01, -2.0958e-01,\n",
       "          -4.1184e-01, -7.6024e-01, -6.9685e-01,  1.5107e-01, -2.1169e+00,\n",
       "          -1.0798e+00, -1.2443e+00,  1.0516e-01, -3.7107e-01,  1.5838e+00,\n",
       "          -6.5819e-01,  1.7613e+00, -1.3358e+00, -5.4824e-02, -3.1665e-01,\n",
       "          -2.8837e-01,  8.6719e-01, -1.1353e+00, -1.8099e-01, -7.5405e-01]]),\n",
       " tensor([-1.5177,  0.1533, -0.8238,  0.8009,  1.3414, -0.6018, -0.5030, -1.5450,\n",
       "          0.1799, -0.1736, -0.1559,  0.7514, -0.4487, -1.0941,  0.3973, -0.7712,\n",
       "          0.0939, -1.4644,  2.0347,  2.0383,  0.9767,  0.9431,  0.1268, -0.4864,\n",
       "          1.9714,  1.1792,  0.2926,  0.2039, -1.2107, -0.6143, -0.0634,  0.5822,\n",
       "          1.5300,  0.2937,  0.8343, -0.6418,  0.0304,  0.1358,  0.5275, -2.3256,\n",
       "         -0.1864,  0.7164, -0.4008, -0.4033,  1.3479,  0.2218,  0.1237, -0.2569,\n",
       "         -0.3393, -1.5003,  0.1005,  1.2767, -0.2517, -0.8290, -0.2669,  1.1184,\n",
       "          0.1326,  0.0903,  0.0298, -0.0501,  0.2606, -0.1395,  1.9102, -0.1056,\n",
       "         -0.0760,  0.2983,  0.4454,  0.0439,  0.3862, -0.9479,  1.1778,  0.8580,\n",
       "         -1.0000, -1.1802, -1.1485, -0.2901, -0.6404,  1.5349,  1.3566, -0.7749,\n",
       "          0.2858,  1.5791,  0.1895,  0.1253,  1.6157,  1.2630, -1.4383, -0.4273,\n",
       "         -1.8039, -1.1234,  0.4639,  0.0137,  0.9296,  0.2605,  0.5511, -0.0589,\n",
       "         -0.2690,  0.4617,  1.7731,  0.9997]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Randomly initialized weights and biases\n",
    "W1 = torch.randn((6, 100))\n",
    "b1 = torch.randn(100)\n",
    "W1, b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad21816c-2252-4375-98cf-a57d7d0239c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 100]), torch.Size([100]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1.shape, b1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82f6f2de-cb9b-47af-b747-ee0d4850176a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.0930, -0.4268],\n",
       "         [-1.0930, -0.4268],\n",
       "         [-1.0930, -0.4268],\n",
       "         [-0.3277,  1.2833],\n",
       "         [ 1.5099,  0.1672],\n",
       "         [-1.0930, -0.4268],\n",
       "         [-1.0930, -0.4268],\n",
       "         [-1.0930, -0.4268],\n",
       "         [ 0.3375, -1.0636],\n",
       "         [-1.6998, -0.3647],\n",
       "         [ 1.0277,  1.3164],\n",
       "         [ 1.9682, -1.6132],\n",
       "         [-1.0930, -0.4268],\n",
       "         [-1.0930, -0.4268],\n",
       "         [-1.0930, -0.4268],\n",
       "         [ 0.0820, -0.0620],\n",
       "         [-1.0930, -0.4268],\n",
       "         [-1.0930, -0.4268],\n",
       "         [-1.0930, -0.4268],\n",
       "         [ 1.0277,  1.3164],\n",
       "         [-0.3678, -1.2620],\n",
       "         [ 0.0820, -0.0620],\n",
       "         [-0.2449, -0.0207],\n",
       "         [-0.3277,  1.2833],\n",
       "         [-1.6998, -0.3647],\n",
       "         [-1.0930, -0.4268],\n",
       "         [-1.0930, -0.4268],\n",
       "         [-1.0930, -0.4268],\n",
       "         [-0.3678, -1.2620],\n",
       "         [ 0.3375, -1.0636],\n",
       "         [-0.0305,  0.5801],\n",
       "         [ 1.3762, -0.1978]]),\n",
       " tensor([[-1.0930, -0.4268],\n",
       "         [-1.0930, -0.4268],\n",
       "         [-0.3277,  1.2833],\n",
       "         [ 1.5099,  0.1672],\n",
       "         [ 1.5099,  0.1672],\n",
       "         [-1.0930, -0.4268],\n",
       "         [-1.0930, -0.4268],\n",
       "         [ 0.3375, -1.0636],\n",
       "         [-1.6998, -0.3647],\n",
       "         [ 1.0277,  1.3164],\n",
       "         [ 1.9682, -1.6132],\n",
       "         [ 1.0277,  1.3164],\n",
       "         [-1.0930, -0.4268],\n",
       "         [-1.0930, -0.4268],\n",
       "         [ 0.0820, -0.0620],\n",
       "         [ 1.9682, -1.6132],\n",
       "         [-1.0930, -0.4268],\n",
       "         [-1.0930, -0.4268],\n",
       "         [ 1.0277,  1.3164],\n",
       "         [-0.3678, -1.2620],\n",
       "         [ 0.0820, -0.0620],\n",
       "         [-0.2449, -0.0207],\n",
       "         [-0.3277,  1.2833],\n",
       "         [-1.6998, -0.3647],\n",
       "         [-1.6998, -0.3647],\n",
       "         [-1.0930, -0.4268],\n",
       "         [-1.0930, -0.4268],\n",
       "         [-0.3678, -1.2620],\n",
       "         [ 0.3375, -1.0636],\n",
       "         [-0.0305,  0.5801],\n",
       "         [ 1.3762, -0.1978],\n",
       "         [ 1.0277,  1.3164]]),\n",
       " tensor([[-1.0930, -0.4268],\n",
       "         [-0.3277,  1.2833],\n",
       "         [ 1.5099,  0.1672],\n",
       "         [ 1.5099,  0.1672],\n",
       "         [ 0.0820, -0.0620],\n",
       "         [-1.0930, -0.4268],\n",
       "         [ 0.3375, -1.0636],\n",
       "         [-1.6998, -0.3647],\n",
       "         [ 1.0277,  1.3164],\n",
       "         [ 1.9682, -1.6132],\n",
       "         [ 1.0277,  1.3164],\n",
       "         [ 0.0820, -0.0620],\n",
       "         [-1.0930, -0.4268],\n",
       "         [ 0.0820, -0.0620],\n",
       "         [ 1.9682, -1.6132],\n",
       "         [ 0.0820, -0.0620],\n",
       "         [-1.0930, -0.4268],\n",
       "         [ 1.0277,  1.3164],\n",
       "         [-0.3678, -1.2620],\n",
       "         [ 0.0820, -0.0620],\n",
       "         [-0.2449, -0.0207],\n",
       "         [-0.3277,  1.2833],\n",
       "         [-1.6998, -0.3647],\n",
       "         [-1.6998, -0.3647],\n",
       "         [ 0.0820, -0.0620],\n",
       "         [-1.0930, -0.4268],\n",
       "         [-0.3678, -1.2620],\n",
       "         [ 0.3375, -1.0636],\n",
       "         [-0.0305,  0.5801],\n",
       "         [ 1.3762, -0.1978],\n",
       "         [ 1.0277,  1.3164],\n",
       "         [ 0.0820, -0.0620]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 Input nueron example\n",
    "emb[:, 0, :], emb[:, 1, :], emb[:, 2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3216706-8317-495a-bb39-30cdaa035122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0930, -0.4268, -1.0930, -0.4268, -1.0930, -0.4268],\n",
       "        [-1.0930, -0.4268, -1.0930, -0.4268, -0.3277,  1.2833],\n",
       "        [-1.0930, -0.4268, -0.3277,  1.2833,  1.5099,  0.1672],\n",
       "        [-0.3277,  1.2833,  1.5099,  0.1672,  1.5099,  0.1672],\n",
       "        [ 1.5099,  0.1672,  1.5099,  0.1672,  0.0820, -0.0620],\n",
       "        [-1.0930, -0.4268, -1.0930, -0.4268, -1.0930, -0.4268],\n",
       "        [-1.0930, -0.4268, -1.0930, -0.4268,  0.3375, -1.0636],\n",
       "        [-1.0930, -0.4268,  0.3375, -1.0636, -1.6998, -0.3647],\n",
       "        [ 0.3375, -1.0636, -1.6998, -0.3647,  1.0277,  1.3164],\n",
       "        [-1.6998, -0.3647,  1.0277,  1.3164,  1.9682, -1.6132],\n",
       "        [ 1.0277,  1.3164,  1.9682, -1.6132,  1.0277,  1.3164],\n",
       "        [ 1.9682, -1.6132,  1.0277,  1.3164,  0.0820, -0.0620],\n",
       "        [-1.0930, -0.4268, -1.0930, -0.4268, -1.0930, -0.4268],\n",
       "        [-1.0930, -0.4268, -1.0930, -0.4268,  0.0820, -0.0620],\n",
       "        [-1.0930, -0.4268,  0.0820, -0.0620,  1.9682, -1.6132],\n",
       "        [ 0.0820, -0.0620,  1.9682, -1.6132,  0.0820, -0.0620],\n",
       "        [-1.0930, -0.4268, -1.0930, -0.4268, -1.0930, -0.4268],\n",
       "        [-1.0930, -0.4268, -1.0930, -0.4268,  1.0277,  1.3164],\n",
       "        [-1.0930, -0.4268,  1.0277,  1.3164, -0.3678, -1.2620],\n",
       "        [ 1.0277,  1.3164, -0.3678, -1.2620,  0.0820, -0.0620],\n",
       "        [-0.3678, -1.2620,  0.0820, -0.0620, -0.2449, -0.0207],\n",
       "        [ 0.0820, -0.0620, -0.2449, -0.0207, -0.3277,  1.2833],\n",
       "        [-0.2449, -0.0207, -0.3277,  1.2833, -1.6998, -0.3647],\n",
       "        [-0.3277,  1.2833, -1.6998, -0.3647, -1.6998, -0.3647],\n",
       "        [-1.6998, -0.3647, -1.6998, -0.3647,  0.0820, -0.0620],\n",
       "        [-1.0930, -0.4268, -1.0930, -0.4268, -1.0930, -0.4268],\n",
       "        [-1.0930, -0.4268, -1.0930, -0.4268, -0.3678, -1.2620],\n",
       "        [-1.0930, -0.4268, -0.3678, -1.2620,  0.3375, -1.0636],\n",
       "        [-0.3678, -1.2620,  0.3375, -1.0636, -0.0305,  0.5801],\n",
       "        [ 0.3375, -1.0636, -0.0305,  0.5801,  1.3762, -0.1978],\n",
       "        [-0.0305,  0.5801,  1.3762, -0.1978,  1.0277,  1.3164],\n",
       "        [ 1.3762, -0.1978,  1.0277,  1.3164,  0.0820, -0.0620]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatinating the second dimension of each to match Weight Matrix W1's dimensions\n",
    "torch.cat([emb[:, 0, :], emb[:, 1, :], emb[:, 2, :]], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46d50687-a7c0-49e6-8737-201014c2c840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.0930, -0.4268],\n",
       "         [-1.0930, -0.4268],\n",
       "         [-1.0930, -0.4268],\n",
       "         [-0.3277,  1.2833],\n",
       "         [ 1.5099,  0.1672],\n",
       "         [-1.0930, -0.4268],\n",
       "         [-1.0930, -0.4268],\n",
       "         [-1.0930, -0.4268],\n",
       "         [ 0.3375, -1.0636],\n",
       "         [-1.6998, -0.3647],\n",
       "         [ 1.0277,  1.3164],\n",
       "         [ 1.9682, -1.6132],\n",
       "         [-1.0930, -0.4268],\n",
       "         [-1.0930, -0.4268],\n",
       "         [-1.0930, -0.4268],\n",
       "         [ 0.0820, -0.0620],\n",
       "         [-1.0930, -0.4268],\n",
       "         [-1.0930, -0.4268],\n",
       "         [-1.0930, -0.4268],\n",
       "         [ 1.0277,  1.3164],\n",
       "         [-0.3678, -1.2620],\n",
       "         [ 0.0820, -0.0620],\n",
       "         [-0.2449, -0.0207],\n",
       "         [-0.3277,  1.2833],\n",
       "         [-1.6998, -0.3647],\n",
       "         [-1.0930, -0.4268],\n",
       "         [-1.0930, -0.4268],\n",
       "         [-1.0930, -0.4268],\n",
       "         [-0.3678, -1.2620],\n",
       "         [ 0.3375, -1.0636],\n",
       "         [-0.0305,  0.5801],\n",
       "         [ 1.3762, -0.1978]]),\n",
       " tensor([[-1.0930, -0.4268],\n",
       "         [-1.0930, -0.4268],\n",
       "         [-0.3277,  1.2833],\n",
       "         [ 1.5099,  0.1672],\n",
       "         [ 1.5099,  0.1672],\n",
       "         [-1.0930, -0.4268],\n",
       "         [-1.0930, -0.4268],\n",
       "         [ 0.3375, -1.0636],\n",
       "         [-1.6998, -0.3647],\n",
       "         [ 1.0277,  1.3164],\n",
       "         [ 1.9682, -1.6132],\n",
       "         [ 1.0277,  1.3164],\n",
       "         [-1.0930, -0.4268],\n",
       "         [-1.0930, -0.4268],\n",
       "         [ 0.0820, -0.0620],\n",
       "         [ 1.9682, -1.6132],\n",
       "         [-1.0930, -0.4268],\n",
       "         [-1.0930, -0.4268],\n",
       "         [ 1.0277,  1.3164],\n",
       "         [-0.3678, -1.2620],\n",
       "         [ 0.0820, -0.0620],\n",
       "         [-0.2449, -0.0207],\n",
       "         [-0.3277,  1.2833],\n",
       "         [-1.6998, -0.3647],\n",
       "         [-1.6998, -0.3647],\n",
       "         [-1.0930, -0.4268],\n",
       "         [-1.0930, -0.4268],\n",
       "         [-0.3678, -1.2620],\n",
       "         [ 0.3375, -1.0636],\n",
       "         [-0.0305,  0.5801],\n",
       "         [ 1.3762, -0.1978],\n",
       "         [ 1.0277,  1.3164]]),\n",
       " tensor([[-1.0930, -0.4268],\n",
       "         [-0.3277,  1.2833],\n",
       "         [ 1.5099,  0.1672],\n",
       "         [ 1.5099,  0.1672],\n",
       "         [ 0.0820, -0.0620],\n",
       "         [-1.0930, -0.4268],\n",
       "         [ 0.3375, -1.0636],\n",
       "         [-1.6998, -0.3647],\n",
       "         [ 1.0277,  1.3164],\n",
       "         [ 1.9682, -1.6132],\n",
       "         [ 1.0277,  1.3164],\n",
       "         [ 0.0820, -0.0620],\n",
       "         [-1.0930, -0.4268],\n",
       "         [ 0.0820, -0.0620],\n",
       "         [ 1.9682, -1.6132],\n",
       "         [ 0.0820, -0.0620],\n",
       "         [-1.0930, -0.4268],\n",
       "         [ 1.0277,  1.3164],\n",
       "         [-0.3678, -1.2620],\n",
       "         [ 0.0820, -0.0620],\n",
       "         [-0.2449, -0.0207],\n",
       "         [-0.3277,  1.2833],\n",
       "         [-1.6998, -0.3647],\n",
       "         [-1.6998, -0.3647],\n",
       "         [ 0.0820, -0.0620],\n",
       "         [-1.0930, -0.4268],\n",
       "         [-0.3678, -1.2620],\n",
       "         [ 0.3375, -1.0636],\n",
       "         [-0.0305,  0.5801],\n",
       "         [ 1.3762, -0.1978],\n",
       "         [ 1.0277,  1.3164],\n",
       "         [ 0.0820, -0.0620]]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of torch.unbind\n",
    "torch.unbind(emb, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8451edb3-e78f-4e63-9186-74a7c0a7c79b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0930, -0.4268, -1.0930, -0.4268, -1.0930, -0.4268],\n",
       "        [-1.0930, -0.4268, -1.0930, -0.4268, -0.3277,  1.2833],\n",
       "        [-1.0930, -0.4268, -0.3277,  1.2833,  1.5099,  0.1672],\n",
       "        [-0.3277,  1.2833,  1.5099,  0.1672,  1.5099,  0.1672],\n",
       "        [ 1.5099,  0.1672,  1.5099,  0.1672,  0.0820, -0.0620],\n",
       "        [-1.0930, -0.4268, -1.0930, -0.4268, -1.0930, -0.4268],\n",
       "        [-1.0930, -0.4268, -1.0930, -0.4268,  0.3375, -1.0636],\n",
       "        [-1.0930, -0.4268,  0.3375, -1.0636, -1.6998, -0.3647],\n",
       "        [ 0.3375, -1.0636, -1.6998, -0.3647,  1.0277,  1.3164],\n",
       "        [-1.6998, -0.3647,  1.0277,  1.3164,  1.9682, -1.6132],\n",
       "        [ 1.0277,  1.3164,  1.9682, -1.6132,  1.0277,  1.3164],\n",
       "        [ 1.9682, -1.6132,  1.0277,  1.3164,  0.0820, -0.0620],\n",
       "        [-1.0930, -0.4268, -1.0930, -0.4268, -1.0930, -0.4268],\n",
       "        [-1.0930, -0.4268, -1.0930, -0.4268,  0.0820, -0.0620],\n",
       "        [-1.0930, -0.4268,  0.0820, -0.0620,  1.9682, -1.6132],\n",
       "        [ 0.0820, -0.0620,  1.9682, -1.6132,  0.0820, -0.0620],\n",
       "        [-1.0930, -0.4268, -1.0930, -0.4268, -1.0930, -0.4268],\n",
       "        [-1.0930, -0.4268, -1.0930, -0.4268,  1.0277,  1.3164],\n",
       "        [-1.0930, -0.4268,  1.0277,  1.3164, -0.3678, -1.2620],\n",
       "        [ 1.0277,  1.3164, -0.3678, -1.2620,  0.0820, -0.0620],\n",
       "        [-0.3678, -1.2620,  0.0820, -0.0620, -0.2449, -0.0207],\n",
       "        [ 0.0820, -0.0620, -0.2449, -0.0207, -0.3277,  1.2833],\n",
       "        [-0.2449, -0.0207, -0.3277,  1.2833, -1.6998, -0.3647],\n",
       "        [-0.3277,  1.2833, -1.6998, -0.3647, -1.6998, -0.3647],\n",
       "        [-1.6998, -0.3647, -1.6998, -0.3647,  0.0820, -0.0620],\n",
       "        [-1.0930, -0.4268, -1.0930, -0.4268, -1.0930, -0.4268],\n",
       "        [-1.0930, -0.4268, -1.0930, -0.4268, -0.3678, -1.2620],\n",
       "        [-1.0930, -0.4268, -0.3678, -1.2620,  0.3375, -1.0636],\n",
       "        [-0.3678, -1.2620,  0.3375, -1.0636, -0.0305,  0.5801],\n",
       "        [ 0.3375, -1.0636, -0.0305,  0.5801,  1.3762, -0.1978],\n",
       "        [-0.0305,  0.5801,  1.3762, -0.1978,  1.0277,  1.3164],\n",
       "        [ 1.3762, -0.1978,  1.0277,  1.3164,  0.0820, -0.0620]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using cat and unvind together to avoid hardcoding dimensions and match emb and W1's dimensions\n",
    "torch.cat(torch.unbind(emb , 1), 1)\n",
    "# Output is same as torch.cat([emb[:, 0, :], emb[:, 1, :], emb[:, 2, :]], 1) but neurons and dimensions are not hard-coded, therefore is better for variable block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "baf798a3-0ebc-4992-9c34-e171453ebed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0930, -0.4268, -1.0930, -0.4268, -1.0930, -0.4268],\n",
       "        [-1.0930, -0.4268, -1.0930, -0.4268, -0.3277,  1.2833],\n",
       "        [-1.0930, -0.4268, -0.3277,  1.2833,  1.5099,  0.1672],\n",
       "        [-0.3277,  1.2833,  1.5099,  0.1672,  1.5099,  0.1672],\n",
       "        [ 1.5099,  0.1672,  1.5099,  0.1672,  0.0820, -0.0620],\n",
       "        [-1.0930, -0.4268, -1.0930, -0.4268, -1.0930, -0.4268],\n",
       "        [-1.0930, -0.4268, -1.0930, -0.4268,  0.3375, -1.0636],\n",
       "        [-1.0930, -0.4268,  0.3375, -1.0636, -1.6998, -0.3647],\n",
       "        [ 0.3375, -1.0636, -1.6998, -0.3647,  1.0277,  1.3164],\n",
       "        [-1.6998, -0.3647,  1.0277,  1.3164,  1.9682, -1.6132],\n",
       "        [ 1.0277,  1.3164,  1.9682, -1.6132,  1.0277,  1.3164],\n",
       "        [ 1.9682, -1.6132,  1.0277,  1.3164,  0.0820, -0.0620],\n",
       "        [-1.0930, -0.4268, -1.0930, -0.4268, -1.0930, -0.4268],\n",
       "        [-1.0930, -0.4268, -1.0930, -0.4268,  0.0820, -0.0620],\n",
       "        [-1.0930, -0.4268,  0.0820, -0.0620,  1.9682, -1.6132],\n",
       "        [ 0.0820, -0.0620,  1.9682, -1.6132,  0.0820, -0.0620],\n",
       "        [-1.0930, -0.4268, -1.0930, -0.4268, -1.0930, -0.4268],\n",
       "        [-1.0930, -0.4268, -1.0930, -0.4268,  1.0277,  1.3164],\n",
       "        [-1.0930, -0.4268,  1.0277,  1.3164, -0.3678, -1.2620],\n",
       "        [ 1.0277,  1.3164, -0.3678, -1.2620,  0.0820, -0.0620],\n",
       "        [-0.3678, -1.2620,  0.0820, -0.0620, -0.2449, -0.0207],\n",
       "        [ 0.0820, -0.0620, -0.2449, -0.0207, -0.3277,  1.2833],\n",
       "        [-0.2449, -0.0207, -0.3277,  1.2833, -1.6998, -0.3647],\n",
       "        [-0.3277,  1.2833, -1.6998, -0.3647, -1.6998, -0.3647],\n",
       "        [-1.6998, -0.3647, -1.6998, -0.3647,  0.0820, -0.0620],\n",
       "        [-1.0930, -0.4268, -1.0930, -0.4268, -1.0930, -0.4268],\n",
       "        [-1.0930, -0.4268, -1.0930, -0.4268, -0.3678, -1.2620],\n",
       "        [-1.0930, -0.4268, -0.3678, -1.2620,  0.3375, -1.0636],\n",
       "        [-0.3678, -1.2620,  0.3375, -1.0636, -0.0305,  0.5801],\n",
       "        [ 0.3375, -1.0636, -0.0305,  0.5801,  1.3762, -0.1978],\n",
       "        [-0.0305,  0.5801,  1.3762, -0.1978,  1.0277,  1.3164],\n",
       "        [ 1.3762, -0.1978,  1.0277,  1.3164,  0.0820, -0.0620]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Efficient way is to use torch.view instead of cat and/or unbind\n",
    "emb.view(emb.shape[0], 6)\n",
    "# Output is same as torch.cat(torch.unbind(emb , 1), 1) but is much more efficient than cat as .view() manipulates 'storage' component of neurons instead of creating new neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "094ab7f8-0910-43e5-b0df-b7b138049b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.7593,  0.4408, -0.9922,  ..., -0.9957,  0.9742,  0.4449],\n",
       "         [-0.9940,  0.0386, -0.9554,  ..., -0.9973,  0.8146, -0.0189],\n",
       "         [-0.9945, -0.9932, -0.6448,  ...,  0.9981, -0.9166,  0.9945],\n",
       "         ...,\n",
       "         [-0.9963, -0.9674,  0.3581,  ...,  0.9990, -0.6609,  1.0000],\n",
       "         [-0.7238, -0.4783,  0.4331,  ...,  0.9972,  0.9661, -0.9518],\n",
       "         [-0.7426,  0.2670,  0.7686,  ...,  0.9811,  0.7985,  0.9954]]),\n",
       " torch.Size([32, 100]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating second layer (hidden layer of activations) of NN \n",
    "h = torch.tanh(emb.view(emb.shape[0], 6) @ W1 + b1)\n",
    "h, h.shape\n",
    "# Keep in mind broadcasting rules for b1. Not to worry in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6456cee-0f13-4af4-ab7f-8cf509bd48e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating second layer with 27 possible outcome\n",
    "W2 = torch.randn((100, 27))\n",
    "b2 = torch.randn(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd1252cf-4740-40ba-8f77-9d5e7a49bc2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = h @ W2 + b2\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "89cf6d1c-26d5-4fb6-a9e1-1d9c4ab5cd91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.3981e-01, 9.3056e+00, 4.7516e-10, 4.6866e-08, 3.5676e+00, 7.3368e-01,\n",
       "         4.4100e-07, 5.2127e-02, 5.1590e+00, 8.7047e-07, 5.6522e-05, 3.4657e-04,\n",
       "         8.0022e-01, 1.2409e+01, 2.1269e+03, 1.4837e-03, 5.8053e+04, 1.5400e+01,\n",
       "         1.6193e+01, 3.6254e+03, 3.9074e+04, 4.0026e+04, 4.1695e-01, 1.1853e-08,\n",
       "         3.8352e-01, 7.9293e+05, 7.7682e-01],\n",
       "        [8.9424e-04, 7.7803e-01, 4.4585e-09, 8.8474e-07, 6.5996e-01, 2.2033e-05,\n",
       "         1.5930e-05, 1.3936e-05, 9.2642e-03, 1.6585e-07, 3.3683e-05, 8.0366e-05,\n",
       "         1.9594e-04, 1.2439e+00, 3.4001e+01, 5.8990e-02, 2.4080e+06, 2.0585e+05,\n",
       "         7.9728e-01, 1.0928e+01, 4.8401e+01, 3.4962e+03, 1.7808e-02, 6.3355e-07,\n",
       "         9.8828e+00, 5.6710e+04, 1.1120e+00],\n",
       "        [1.5111e+02, 6.7252e+00, 4.7367e-02, 1.8843e+07, 1.3638e+03, 3.5349e-01,\n",
       "         1.6381e-01, 1.0333e-02, 1.9335e-04, 8.7114e-01, 1.3176e+01, 1.5410e-02,\n",
       "         2.4294e-03, 4.2170e-02, 4.5514e-05, 1.9417e-01, 1.0433e+01, 1.2120e+01,\n",
       "         3.0940e+01, 5.1263e-02, 1.3335e-04, 2.1350e-06, 4.1316e-02, 2.8782e+02,\n",
       "         1.8713e+04, 4.8332e+06, 3.3050e-01],\n",
       "        [5.1882e+00, 1.5383e+03, 2.5430e+03, 8.3401e+01, 2.2937e+03, 9.3195e+03,\n",
       "         2.7972e+00, 2.5965e-06, 1.4186e-02, 1.1015e+06, 4.0329e-02, 1.0780e+02,\n",
       "         2.0242e+04, 1.0875e+02, 9.9593e-07, 4.9457e+03, 4.5478e-05, 5.8625e+00,\n",
       "         1.5487e-02, 4.5000e-05, 1.6428e-02, 4.1598e-12, 1.7881e-02, 1.5731e+08,\n",
       "         1.2500e-01, 4.2179e+00, 4.4834e+02],\n",
       "        [4.3015e-02, 1.7207e+04, 5.2184e+08, 1.1947e+03, 4.2032e+02, 5.9853e+02,\n",
       "         5.8233e+02, 1.3402e-04, 2.0227e+02, 3.2759e+07, 1.6281e-01, 3.2959e-01,\n",
       "         1.6218e-01, 1.7981e+06, 2.7104e+00, 5.1283e+06, 3.3917e-04, 6.1864e-05,\n",
       "         4.6102e+00, 5.7730e+01, 7.9319e+00, 9.6157e-07, 6.4674e+00, 2.1029e+04,\n",
       "         3.2736e-04, 7.9098e-03, 5.2107e+02],\n",
       "        [2.3981e-01, 9.3056e+00, 4.7516e-10, 4.6866e-08, 3.5676e+00, 7.3368e-01,\n",
       "         4.4100e-07, 5.2127e-02, 5.1590e+00, 8.7047e-07, 5.6522e-05, 3.4657e-04,\n",
       "         8.0022e-01, 1.2409e+01, 2.1269e+03, 1.4837e-03, 5.8053e+04, 1.5400e+01,\n",
       "         1.6193e+01, 3.6254e+03, 3.9074e+04, 4.0026e+04, 4.1695e-01, 1.1853e-08,\n",
       "         3.8352e-01, 7.9293e+05, 7.7682e-01],\n",
       "        [1.6205e+01, 6.3939e-01, 7.7622e-08, 2.4079e-06, 2.8048e+01, 1.8438e+00,\n",
       "         1.1739e-03, 4.6757e-03, 1.5284e+00, 3.2294e-01, 9.9156e-01, 9.9316e-01,\n",
       "         1.2757e+02, 1.1784e-05, 1.6727e+03, 2.0416e-05, 1.0181e+04, 6.5126e-02,\n",
       "         5.0431e+03, 6.0874e+04, 1.7500e+02, 2.7269e+02, 6.2679e-01, 1.1857e-06,\n",
       "         1.6710e+01, 1.4022e+03, 6.0981e-04],\n",
       "        [1.8940e-05, 3.5034e+02, 4.0178e-04, 2.4819e-10, 5.1848e+04, 1.7993e-01,\n",
       "         3.7304e-04, 1.9762e-04, 4.1899e+00, 3.8972e-05, 1.3901e-07, 1.3811e-06,\n",
       "         4.4987e+01, 1.3258e-01, 1.5783e+00, 7.5963e+01, 3.8003e-01, 2.3320e+01,\n",
       "         6.8149e+01, 3.7046e+04, 4.1836e+04, 3.2851e+02, 1.8257e-01, 7.0242e-05,\n",
       "         6.4471e-02, 2.5716e+04, 2.0615e+03],\n",
       "        [1.0097e-03, 9.5939e+03, 2.9089e-07, 3.5702e-03, 5.6888e-03, 4.8162e-09,\n",
       "         5.0971e-04, 1.0171e+00, 7.5264e-01, 1.7787e-05, 3.9254e+04, 1.0156e+02,\n",
       "         5.6759e-05, 1.1092e+00, 3.7055e+11, 1.6855e-02, 1.5345e+08, 1.1345e+00,\n",
       "         1.0779e+03, 3.4596e+03, 1.4927e+00, 8.6558e+01, 4.3602e-01, 1.1818e-01,\n",
       "         1.2748e+01, 2.9869e+03, 1.2297e-03],\n",
       "        [2.8822e+03, 3.5494e+04, 1.4593e+01, 1.3831e+02, 1.3902e+05, 2.3642e+03,\n",
       "         1.6778e+00, 1.8932e-02, 9.5382e-03, 3.8322e+01, 2.9339e+02, 1.4425e+00,\n",
       "         8.5842e-01, 1.5028e-04, 1.4082e-05, 3.7241e-03, 7.0107e-06, 1.2938e-01,\n",
       "         5.4342e+02, 1.2809e-02, 1.0744e-03, 5.8386e-07, 4.6293e-01, 1.1788e+02,\n",
       "         1.7805e-01, 1.1626e+00, 1.9528e-01],\n",
       "        [5.7972e-02, 1.7683e+07, 2.6856e+05, 5.0623e-01, 9.4859e+03, 1.1096e+03,\n",
       "         4.8146e+00, 2.0574e-06, 2.2867e+02, 4.9652e+09, 4.3325e-04, 1.0657e+01,\n",
       "         2.2342e+00, 9.0300e+01, 1.6830e-02, 7.7501e+03, 2.1349e-06, 4.8639e-03,\n",
       "         1.3297e-02, 3.5416e+00, 4.5685e-02, 5.4450e-07, 5.7759e-02, 1.4633e+05,\n",
       "         2.3743e-03, 1.6096e-04, 5.5057e-02],\n",
       "        [2.0153e-02, 2.7545e+05, 2.5746e+07, 2.4803e+04, 6.8029e-01, 5.4978e+03,\n",
       "         1.4916e+03, 6.1241e-03, 1.2353e-03, 2.1020e-03, 1.9601e+02, 9.8191e-03,\n",
       "         9.9030e+00, 4.1352e+04, 4.6715e+03, 2.6894e+01, 4.8949e-03, 6.2525e-02,\n",
       "         2.8013e+01, 4.4152e-01, 6.5121e-01, 1.2939e-05, 2.2173e+01, 8.3295e+00,\n",
       "         8.6524e-04, 1.2854e-01, 1.0976e+02],\n",
       "        [2.3981e-01, 9.3056e+00, 4.7516e-10, 4.6866e-08, 3.5676e+00, 7.3368e-01,\n",
       "         4.4100e-07, 5.2127e-02, 5.1590e+00, 8.7047e-07, 5.6522e-05, 3.4657e-04,\n",
       "         8.0022e-01, 1.2409e+01, 2.1269e+03, 1.4837e-03, 5.8053e+04, 1.5400e+01,\n",
       "         1.6193e+01, 3.6254e+03, 3.9074e+04, 4.0026e+04, 4.1695e-01, 1.1853e-08,\n",
       "         3.8352e-01, 7.9293e+05, 7.7682e-01],\n",
       "        [1.1138e-01, 1.4132e+00, 2.8957e-10, 1.3374e-06, 1.8907e-01, 4.3997e-03,\n",
       "         1.4204e-05, 1.0791e-03, 1.0431e-01, 4.1006e-04, 1.0022e-01, 6.7245e-03,\n",
       "         5.0444e-01, 1.9071e+00, 5.3186e+03, 1.2274e-04, 2.9926e+05, 5.3142e+01,\n",
       "         4.0420e+02, 2.5611e+02, 1.0000e+03, 9.0321e+03, 1.0869e-02, 1.5688e-07,\n",
       "         5.7153e+01, 2.2143e+05, 1.8123e-02],\n",
       "        [8.1240e-01, 3.1967e+03, 1.7853e-03, 3.9683e+00, 3.1112e+05, 9.0691e+01,\n",
       "         5.6013e-03, 1.7750e-03, 3.7372e+00, 8.2571e+06, 1.5257e+02, 5.8265e+04,\n",
       "         2.6672e+02, 4.1869e-05, 3.8738e+01, 1.6129e-06, 1.0705e-03, 1.8096e-03,\n",
       "         8.8833e+03, 1.3039e+01, 1.0513e+00, 4.8411e-07, 1.0021e+01, 9.2218e+01,\n",
       "         7.6289e-01, 1.0521e+01, 4.7433e-06],\n",
       "        [1.4093e-04, 6.8950e+04, 4.3669e-01, 7.8493e-03, 6.0896e+00, 3.9051e+06,\n",
       "         1.3175e+00, 1.7265e-05, 3.2674e+02, 6.9931e+08, 2.3664e-06, 1.1341e+02,\n",
       "         2.6075e+03, 2.7729e-01, 2.4428e-01, 1.3147e+07, 1.0438e-05, 2.4352e-05,\n",
       "         1.3201e+01, 7.8599e-01, 1.7452e+00, 9.6427e-05, 2.4049e+01, 6.2122e+04,\n",
       "         1.1537e-07, 2.4287e-03, 7.2370e-02],\n",
       "        [2.3981e-01, 9.3056e+00, 4.7516e-10, 4.6866e-08, 3.5676e+00, 7.3368e-01,\n",
       "         4.4100e-07, 5.2127e-02, 5.1590e+00, 8.7047e-07, 5.6522e-05, 3.4657e-04,\n",
       "         8.0022e-01, 1.2409e+01, 2.1269e+03, 1.4837e-03, 5.8053e+04, 1.5400e+01,\n",
       "         1.6193e+01, 3.6254e+03, 3.9074e+04, 4.0026e+04, 4.1695e-01, 1.1853e-08,\n",
       "         3.8352e-01, 7.9293e+05, 7.7682e-01],\n",
       "        [5.0868e-04, 7.1923e-01, 3.1067e-08, 7.5949e-04, 1.4177e+01, 1.2363e-07,\n",
       "         1.0326e-03, 3.5292e-05, 5.1532e-01, 4.7350e-04, 9.6150e+01, 1.5198e-04,\n",
       "         5.5364e-05, 2.7029e-01, 1.2282e+05, 1.3351e-02, 1.9854e+04, 1.0311e+02,\n",
       "         2.7557e+02, 2.9883e+02, 7.1821e+01, 2.3912e-01, 5.1131e-03, 5.1335e-05,\n",
       "         1.8948e+03, 5.2002e+04, 9.2781e-04],\n",
       "        [9.8845e-01, 2.1035e+02, 7.0615e+05, 4.0937e-02, 2.9910e+02, 1.9234e+07,\n",
       "         4.9860e+00, 2.6410e-03, 6.3332e-03, 3.7187e-02, 1.7579e-01, 8.4699e-07,\n",
       "         4.8402e+02, 5.8026e-01, 5.8389e-08, 2.0472e+03, 2.5279e-03, 9.2707e+02,\n",
       "         1.0174e+00, 3.8515e-05, 6.2467e-02, 2.0471e-02, 1.7823e+03, 2.7278e-01,\n",
       "         4.0338e-02, 6.3299e+01, 4.5172e-01],\n",
       "        [2.2628e-02, 2.1953e+03, 4.2330e-03, 9.4335e+00, 1.4888e+01, 7.9322e-01,\n",
       "         6.4704e-05, 1.0522e-04, 6.6536e+00, 1.4786e+05, 2.2086e-06, 1.8257e+03,\n",
       "         1.3321e+03, 4.3658e+03, 2.9504e+05, 1.4372e+03, 3.3984e+00, 4.0741e-03,\n",
       "         3.5271e+00, 1.6731e+05, 7.1592e+04, 6.9676e-03, 4.1099e-02, 6.7073e+00,\n",
       "         6.6252e+01, 3.6681e-03, 2.8966e-04],\n",
       "        [1.9711e-05, 2.8551e+02, 1.4881e-02, 9.4833e-08, 4.0805e-02, 2.7466e+03,\n",
       "         2.9332e-02, 1.5960e-02, 2.4608e-01, 3.0578e-04, 1.4747e-03, 2.0495e-05,\n",
       "         3.0618e+01, 3.1450e+00, 4.8707e-01, 8.2617e-02, 1.0757e+01, 1.3301e-03,\n",
       "         2.7966e+02, 1.0335e-01, 9.0883e+02, 5.7930e-02, 2.3806e+03, 9.1762e-05,\n",
       "         1.9431e-01, 8.3013e+01, 1.6367e+01],\n",
       "        [5.3765e-04, 6.4224e+02, 3.3852e-03, 2.1388e-02, 4.8819e-02, 1.4437e-02,\n",
       "         4.7615e-03, 1.4091e-03, 3.5325e-02, 2.0101e-07, 8.9367e-06, 2.3403e-03,\n",
       "         1.2246e-03, 1.5818e+03, 1.5115e+01, 2.3102e+01, 1.8242e+03, 7.8115e+01,\n",
       "         2.5637e-04, 3.6704e+01, 1.2278e-02, 1.1346e+00, 2.3303e-04, 2.4512e-04,\n",
       "         2.1547e-01, 1.9343e+04, 2.0047e+03],\n",
       "        [6.6542e-01, 1.0147e+00, 1.0194e+01, 2.1940e-03, 5.4997e+00, 4.3926e+00,\n",
       "         1.1241e-08, 1.1788e-01, 2.2399e-04, 1.3813e-10, 2.7827e-07, 3.2960e-04,\n",
       "         1.2916e+01, 1.7584e+00, 1.4790e-04, 2.3903e+02, 5.5431e+01, 3.9163e+06,\n",
       "         9.8261e-05, 2.2958e+00, 4.8298e+01, 2.0009e+06, 3.5259e-01, 1.4869e-08,\n",
       "         1.5731e-03, 5.5370e+02, 2.0510e+02],\n",
       "        [9.8693e-01, 9.8555e-04, 1.9208e-04, 1.5939e-07, 3.3783e+00, 5.6540e-04,\n",
       "         8.1335e-10, 1.9807e-03, 5.9104e+00, 6.7692e-07, 1.2410e-04, 1.0134e-01,\n",
       "         2.5306e+02, 6.3743e+03, 2.9345e+03, 8.5191e+01, 2.0792e+06, 1.5661e+07,\n",
       "         1.8783e-04, 5.1327e+02, 4.6586e+02, 1.2104e+03, 2.6038e-02, 1.4192e-08,\n",
       "         4.8602e-01, 1.3863e+03, 1.5735e-02],\n",
       "        [5.5067e+00, 3.2462e-01, 2.0919e-12, 2.7233e-06, 1.8473e-01, 1.3270e-03,\n",
       "         2.1342e-05, 1.5092e-02, 4.2360e-02, 2.1985e-03, 9.3236e+00, 9.2667e-02,\n",
       "         5.9248e-01, 3.4373e+01, 3.7492e+02, 5.1106e-06, 6.8472e+05, 4.7331e+04,\n",
       "         4.0380e+01, 4.6240e+01, 2.7631e+02, 1.2541e+05, 1.5644e-02, 5.2961e-09,\n",
       "         1.0611e+03, 7.4884e+05, 8.6709e-05],\n",
       "        [2.3981e-01, 9.3056e+00, 4.7516e-10, 4.6866e-08, 3.5676e+00, 7.3368e-01,\n",
       "         4.4100e-07, 5.2127e-02, 5.1590e+00, 8.7047e-07, 5.6522e-05, 3.4657e-04,\n",
       "         8.0022e-01, 1.2409e+01, 2.1269e+03, 1.4837e-03, 5.8053e+04, 1.5400e+01,\n",
       "         1.6193e+01, 3.6254e+03, 3.9074e+04, 4.0026e+04, 4.1695e-01, 1.1853e-08,\n",
       "         3.8352e-01, 7.9293e+05, 7.7682e-01],\n",
       "        [4.6116e+00, 4.4011e+01, 3.1353e-05, 5.7939e-07, 1.8451e+01, 1.1776e+01,\n",
       "         4.7774e-04, 3.5191e-01, 8.4473e+00, 2.7384e-03, 7.5670e-03, 1.1398e+00,\n",
       "         1.0434e+01, 2.0245e-02, 1.1908e+03, 2.7247e-05, 2.1607e+03, 2.2998e-01,\n",
       "         1.4993e+03, 4.5153e+06, 1.9964e+03, 4.4560e+03, 4.5322e-02, 1.1501e-08,\n",
       "         4.3711e+01, 1.2385e+04, 9.3533e-04],\n",
       "        [2.2137e-01, 6.6244e+00, 5.9920e-06, 2.5397e-07, 4.7043e+00, 1.0244e+04,\n",
       "         1.4996e-01, 9.6641e-05, 4.0231e+01, 1.8356e+04, 4.8646e-03, 1.3989e+00,\n",
       "         6.6177e+04, 2.9702e-04, 7.7514e+02, 1.0739e-02, 8.8674e+00, 9.8031e-06,\n",
       "         5.8834e+05, 9.4326e+05, 4.3406e+03, 8.7695e-02, 2.7988e-01, 5.1248e-02,\n",
       "         5.0512e-01, 6.2898e-01, 2.4344e-04],\n",
       "        [3.8801e-09, 2.6490e+00, 6.3797e-06, 3.6905e-08, 6.4106e-02, 5.8933e+02,\n",
       "         8.9989e-01, 2.6400e-03, 7.8995e+02, 4.5944e-01, 1.2004e-02, 1.8833e-05,\n",
       "         2.4465e-01, 1.0075e+01, 3.1045e+01, 1.2517e-02, 4.8948e-03, 9.7136e-07,\n",
       "         8.4768e+03, 4.6731e-01, 5.6094e+04, 7.1224e-03, 4.0894e+00, 1.1585e-03,\n",
       "         2.7262e-02, 2.7332e-01, 5.4907e+00],\n",
       "        [6.7681e-02, 7.7235e+04, 2.1194e+01, 4.9848e+04, 1.0556e-01, 1.3784e+02,\n",
       "         1.4059e-01, 1.0035e+02, 5.9400e-03, 2.1911e+00, 1.2539e+01, 3.2318e+05,\n",
       "         1.3587e+02, 9.1637e-01, 2.5127e+04, 3.1756e-04, 5.3102e+02, 8.5428e-04,\n",
       "         4.4438e+03, 2.3931e+00, 7.1141e-01, 3.4161e-05, 1.5088e+00, 9.8690e+01,\n",
       "         1.4000e-02, 7.3945e+01, 2.2920e-02],\n",
       "        [9.6979e-04, 1.1951e+04, 7.9569e+03, 3.5850e+02, 4.4862e+02, 1.4058e+05,\n",
       "         3.2200e+00, 1.5798e-06, 7.6670e-04, 3.9153e+03, 1.1709e-03, 1.0894e-04,\n",
       "         5.4189e+03, 4.9506e+01, 2.3415e-08, 2.3990e+03, 4.8937e-06, 6.7526e-02,\n",
       "         9.3616e-03, 1.7754e-03, 1.1591e-03, 1.1207e-10, 1.1937e-01, 8.5901e+04,\n",
       "         5.3721e-01, 4.0889e-01, 1.7876e+02],\n",
       "        [1.7533e+00, 2.4841e+04, 1.2253e+09, 1.4255e+06, 1.3414e+01, 9.5173e+02,\n",
       "         2.2557e+00, 7.1540e-02, 3.7143e-03, 3.3792e-01, 6.3505e-01, 2.4837e-02,\n",
       "         2.5423e+00, 4.9605e+05, 2.0991e+00, 1.4694e+03, 5.1754e-04, 1.6448e+01,\n",
       "         2.6753e-01, 9.5586e-03, 6.7582e-02, 9.9268e-06, 1.4171e+00, 4.9077e+02,\n",
       "         2.5745e-02, 1.8857e-01, 4.7410e+02]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding fake counts for 0 value parameters by exponentiating the parameters\n",
    "counts = logits.exp()\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2bb0b712-93a0-4c66-8ff5-bb5a05fc5522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalising counts by creating probability matrix\n",
    "prob = counts / counts.sum(1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ac289d7-cdfa-407a-b8ce-744d35a4f467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]), tensor(1.))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob.shape, prob[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c672a795-5b9c-45ba-92d5-607e9254fcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating loss manually i.e Negative Log Likelihood\n",
    "loss = -prob[torch.arange(32), Y].log().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5d8cbee-58ae-448c-9511-2078ff0978ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14.8950)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74649351-7db8-493b-ab1a-53305070995c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14.8950)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pytorch introduces a much more efficient and readable way to calculate this same loss using \"Cross Entropy\"\n",
    "loss = F.cross_entropy(logits, Y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "615e159d-9f14-4a32-9baf-8442d1e58a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Arranging everything for readability ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6ef497e3-f983-4848-b068-fbb0610ca824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.Size([32]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c75a4932-12c1-402e-989c-294787696f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647) # for same result reproducibility\n",
    "C = torch.randn((27, 2), generator=g)\n",
    "W1 = torch.randn((6, 100), generator = g)\n",
    "b1 = torch.randn(100, generator = g)\n",
    "W2 = torch.randn((100, 27), generator = g)\n",
    "b2 = torch.randn(27, generator = g)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2ad705fb-aafd-42a0-aaf4-33fb231db227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters) # total number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d7805322-645d-4578-ab5c-d3e5eded9fc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.7697)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X] # Embeddings (32, 3, 2)\n",
    "# First layer will be 3 input embeddings\n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # Second (hidden) Layer (32, 100)\n",
    "logits = h @ W2 + b2 # Third Layer (32, 27)\n",
    "counts = logits.exp() # Fake Counts to prevent inf nll\n",
    "prob = counts / counts.sum(1, keepdims=True) # Probability matrix/ normalised counts\n",
    "loss = -prob[torch.arange(32), Y].log().mean() # Negative Log likelihood loss\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a43f5955-27ef-40be-893b-92db857f26c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.7697)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating loss efficiently using pytorch\n",
    "loss = F.cross_entropy(logits, Y) # We will use this as it is efficient and is better for backpropogation as well\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6aa2eae1-de2e-4c9b-b8c8-76e827bda12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bb449bd3-6ce1-4e5e-986d-af19219dafdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9858498573303223\n",
      "3.6028313636779785\n",
      "3.2621419429779053\n",
      "2.961381435394287\n",
      "2.6982975006103516\n",
      "2.469712495803833\n",
      "2.271660804748535\n",
      "2.1012837886810303\n",
      "1.9571770429611206\n",
      "1.837485671043396\n",
      "1.7380964756011963\n",
      "1.6535117626190186\n",
      "1.5790902376174927\n",
      "1.5117673873901367\n",
      "1.4496052265167236\n",
      "1.391312599182129\n",
      "1.3359929323196411\n",
      "1.2830536365509033\n",
      "1.232191801071167\n",
      "1.1833819150924683\n",
      "1.1367989778518677\n",
      "1.0926649570465088\n",
      "1.0510928630828857\n",
      "1.012027382850647\n",
      "0.9752706289291382\n",
      "0.9405570030212402\n",
      "0.9076128602027893\n",
      "0.8761926293373108\n",
      "0.8460893630981445\n",
      "0.8171359300613403\n",
      "0.7891993522644043\n",
      "0.7621751427650452\n",
      "0.7359815835952759\n",
      "0.710558295249939\n",
      "0.6858614087104797\n",
      "0.6618655323982239\n",
      "0.6385658383369446\n",
      "0.6159822344779968\n",
      "0.594166100025177\n",
      "0.573210597038269\n",
      "0.5532565712928772\n",
      "0.5344884991645813\n",
      "0.5171172618865967\n",
      "0.501331627368927\n",
      "0.4872431457042694\n",
      "0.4748407006263733\n",
      "0.4639979302883148\n",
      "0.4545147120952606\n",
      "0.44617128372192383\n",
      "0.43876662850379944\n",
      "0.43213337659835815\n",
      "0.42613911628723145\n",
      "0.4206799268722534\n",
      "0.41567543148994446\n",
      "0.4110616147518158\n",
      "0.40678733587265015\n",
      "0.4028107821941376\n",
      "0.39909741282463074\n",
      "0.3956182599067688\n",
      "0.3923478126525879\n",
      "0.3892655372619629\n",
      "0.3863520622253418\n",
      "0.3835916817188263\n",
      "0.38097015023231506\n",
      "0.3784741461277008\n",
      "0.37609297037124634\n",
      "0.3738163113594055\n",
      "0.37163496017456055\n",
      "0.36954087018966675\n",
      "0.3675267994403839\n",
      "0.3655855655670166\n",
      "0.3637113869190216\n",
      "0.3618983328342438\n",
      "0.3601416051387787\n",
      "0.3584362268447876\n",
      "0.35677802562713623\n",
      "0.35516268014907837\n",
      "0.3535870313644409\n",
      "0.3520471453666687\n",
      "0.3505396842956543\n",
      "0.3490622043609619\n",
      "0.3476121425628662\n",
      "0.3461865186691284\n",
      "0.34478360414505005\n",
      "0.3434009253978729\n",
      "0.34203672409057617\n",
      "0.34068989753723145\n",
      "0.33935847878456116\n",
      "0.33804184198379517\n",
      "0.3367387354373932\n",
      "0.3354485332965851\n",
      "0.33417102694511414\n",
      "0.33290573954582214\n",
      "0.3316528797149658\n",
      "0.3304123878479004\n",
      "0.32918474078178406\n",
      "0.3279706537723541\n",
      "0.3267703056335449\n",
      "0.32558518648147583\n",
      "0.32441580295562744\n"
     ]
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "    # FORWARD PASS\n",
    "    emb = C[X] # Embeddings (32, 3, 2)\n",
    "    # First layer will be 3 input embeddings\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # Second (hidden) Layer (32, 100)\n",
    "    logits = h @ W2 + b2 # Third Layer (32, 27)\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    print(loss.item())\n",
    "    # BACKWARD PASS\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    for p in parameters:\n",
    "        p.data += -0.1 * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4fbcc876-49fc-4310-bfb4-0905cb76e48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 3\n",
    "X, Y = [], []\n",
    "for w in words: # use all words instead of first 5\n",
    "    context = [0]*block_size\n",
    "    for ch in w+'.':\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        context = context[1:] + [ix]\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f1c47d09-35ea-44d1-ab58-1f39cb786d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([208200,  31568, 190494,  75912,  94504, 138189, 160639, 153969, 104400,\n",
       "        190460, 199338, 220085,  62629,  15161, 155079,  36966, 189953,  14279,\n",
       "        212474,  62998, 157120,  71317, 193494, 180368, 216041,  64664,  25193,\n",
       "         80240, 165418, 156072,  48684, 126411])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The loss is so less because we are overfitting the data. That is, we are feeding just 32 inputs to 3481 parameters\n",
    "# For efficiency we will create batches of data of size 32 like this\n",
    "torch.randint(0, X.shape[0], (32,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a895b10e-d9c8-493f-af96-4a3a266a8ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.816551208496094\n",
      "3.99900484085083\n",
      "3.70257568359375\n",
      "4.36957311630249\n",
      "4.043515205383301\n",
      "3.8395304679870605\n",
      "3.999135971069336\n",
      "3.5969462394714355\n",
      "4.688202381134033\n",
      "3.367837905883789\n",
      "3.7522029876708984\n",
      "4.225220203399658\n",
      "4.26640510559082\n",
      "4.146937847137451\n",
      "4.067058563232422\n",
      "4.324132919311523\n",
      "2.986145496368408\n",
      "3.82460355758667\n",
      "3.3073461055755615\n",
      "4.307409763336182\n",
      "3.848804235458374\n",
      "3.620663642883301\n",
      "2.765113353729248\n",
      "3.7233009338378906\n",
      "3.881305694580078\n",
      "3.719640016555786\n",
      "4.046329498291016\n",
      "4.057267189025879\n",
      "4.291866779327393\n",
      "4.660467147827148\n",
      "4.508404731750488\n",
      "3.689263343811035\n",
      "3.447281837463379\n",
      "4.01051139831543\n",
      "3.8550305366516113\n",
      "3.5758094787597656\n",
      "3.411031484603882\n",
      "4.486516952514648\n",
      "2.9746298789978027\n",
      "3.0283727645874023\n",
      "4.621485710144043\n",
      "3.7561583518981934\n",
      "4.043057918548584\n",
      "3.100371837615967\n",
      "3.966264247894287\n",
      "3.5500662326812744\n",
      "4.232510089874268\n",
      "3.568286418914795\n",
      "3.8858444690704346\n",
      "3.342175245285034\n",
      "3.084225654602051\n",
      "3.87994122505188\n",
      "3.7470390796661377\n",
      "3.8616867065429688\n",
      "3.863570213317871\n",
      "2.920130729675293\n",
      "2.588942289352417\n",
      "3.4325575828552246\n",
      "3.6333441734313965\n",
      "3.5675244331359863\n",
      "3.3202385902404785\n",
      "4.018086910247803\n",
      "3.6277105808258057\n",
      "3.252323865890503\n",
      "4.029691696166992\n",
      "2.7937142848968506\n",
      "3.463223457336426\n",
      "3.272022247314453\n",
      "3.1785078048706055\n",
      "2.894646644592285\n",
      "3.3403964042663574\n",
      "2.8346059322357178\n",
      "3.3092191219329834\n",
      "2.435678482055664\n",
      "3.2445366382598877\n",
      "3.2739455699920654\n",
      "3.436617374420166\n",
      "2.643179178237915\n",
      "2.229727268218994\n",
      "3.3026771545410156\n",
      "3.6545982360839844\n",
      "3.6722445487976074\n",
      "3.1181118488311768\n",
      "2.6588473320007324\n",
      "3.8628199100494385\n",
      "3.3925235271453857\n",
      "3.759577512741089\n",
      "2.8521580696105957\n",
      "3.1104984283447266\n",
      "2.4699649810791016\n",
      "2.8177316188812256\n",
      "3.52830171585083\n",
      "2.961575746536255\n",
      "3.3417305946350098\n",
      "3.4681994915008545\n",
      "4.364424228668213\n",
      "4.071183204650879\n",
      "3.629579782485962\n",
      "3.658132791519165\n",
      "3.8388450145721436\n"
     ]
    }
   ],
   "source": [
    "#Using batches\n",
    "for _ in range(100):\n",
    "    ix = torch.randint(0, X.shape[0], (32,))\n",
    "    # FORWARD PASS\n",
    "    emb = C[X[ix]] # Embeddings (32, 3, 2)\n",
    "    # First layer will be 3 input embeddings\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # Second (hidden) Layer (32, 100)\n",
    "    logits = h @ W2 + b2 # Third Layer (32, 27)\n",
    "    loss = F.cross_entropy(logits, Y[ix])\n",
    "    print(loss.item())\n",
    "    # BACKWARD PASS\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    for p in parameters:\n",
    "        p.data += -0.1 * p.grad\n",
    "# This will be much faster than training the model on all 32000+ words\n",
    "# Even tho the quality of gradient is not that much reliable, it is pretty good if we keep speed in mind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "269fb1fd-d455-41cf-ac2e-04011776d32e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0011,\n",
       "        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n",
       "        0.0011, 0.0011, 0.0011, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012,\n",
       "        0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0013, 0.0013, 0.0013,\n",
       "        0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0014,\n",
       "        0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014,\n",
       "        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n",
       "        0.0015, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
       "        0.0016, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017,\n",
       "        0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0019,\n",
       "        0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0020, 0.0020,\n",
       "        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0021, 0.0021, 0.0021, 0.0021,\n",
       "        0.0021, 0.0021, 0.0021, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "        0.0022, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0024, 0.0024,\n",
       "        0.0024, 0.0024, 0.0024, 0.0024, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
       "        0.0025, 0.0026, 0.0026, 0.0026, 0.0026, 0.0026, 0.0027, 0.0027, 0.0027,\n",
       "        0.0027, 0.0027, 0.0027, 0.0028, 0.0028, 0.0028, 0.0028, 0.0028, 0.0029,\n",
       "        0.0029, 0.0029, 0.0029, 0.0029, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n",
       "        0.0031, 0.0031, 0.0031, 0.0031, 0.0032, 0.0032, 0.0032, 0.0032, 0.0032,\n",
       "        0.0033, 0.0033, 0.0033, 0.0033, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034,\n",
       "        0.0035, 0.0035, 0.0035, 0.0035, 0.0036, 0.0036, 0.0036, 0.0036, 0.0037,\n",
       "        0.0037, 0.0037, 0.0037, 0.0038, 0.0038, 0.0038, 0.0039, 0.0039, 0.0039,\n",
       "        0.0039, 0.0040, 0.0040, 0.0040, 0.0040, 0.0041, 0.0041, 0.0041, 0.0042,\n",
       "        0.0042, 0.0042, 0.0042, 0.0043, 0.0043, 0.0043, 0.0044, 0.0044, 0.0044,\n",
       "        0.0045, 0.0045, 0.0045, 0.0045, 0.0046, 0.0046, 0.0046, 0.0047, 0.0047,\n",
       "        0.0047, 0.0048, 0.0048, 0.0048, 0.0049, 0.0049, 0.0049, 0.0050, 0.0050,\n",
       "        0.0050, 0.0051, 0.0051, 0.0051, 0.0052, 0.0052, 0.0053, 0.0053, 0.0053,\n",
       "        0.0054, 0.0054, 0.0054, 0.0055, 0.0055, 0.0056, 0.0056, 0.0056, 0.0057,\n",
       "        0.0057, 0.0058, 0.0058, 0.0058, 0.0059, 0.0059, 0.0060, 0.0060, 0.0060,\n",
       "        0.0061, 0.0061, 0.0062, 0.0062, 0.0062, 0.0063, 0.0063, 0.0064, 0.0064,\n",
       "        0.0065, 0.0065, 0.0066, 0.0066, 0.0067, 0.0067, 0.0067, 0.0068, 0.0068,\n",
       "        0.0069, 0.0069, 0.0070, 0.0070, 0.0071, 0.0071, 0.0072, 0.0072, 0.0073,\n",
       "        0.0073, 0.0074, 0.0074, 0.0075, 0.0075, 0.0076, 0.0076, 0.0077, 0.0077,\n",
       "        0.0078, 0.0079, 0.0079, 0.0080, 0.0080, 0.0081, 0.0081, 0.0082, 0.0082,\n",
       "        0.0083, 0.0084, 0.0084, 0.0085, 0.0085, 0.0086, 0.0086, 0.0087, 0.0088,\n",
       "        0.0088, 0.0089, 0.0090, 0.0090, 0.0091, 0.0091, 0.0092, 0.0093, 0.0093,\n",
       "        0.0094, 0.0095, 0.0095, 0.0096, 0.0097, 0.0097, 0.0098, 0.0099, 0.0099,\n",
       "        0.0100, 0.0101, 0.0101, 0.0102, 0.0103, 0.0104, 0.0104, 0.0105, 0.0106,\n",
       "        0.0106, 0.0107, 0.0108, 0.0109, 0.0109, 0.0110, 0.0111, 0.0112, 0.0112,\n",
       "        0.0113, 0.0114, 0.0115, 0.0116, 0.0116, 0.0117, 0.0118, 0.0119, 0.0120,\n",
       "        0.0121, 0.0121, 0.0122, 0.0123, 0.0124, 0.0125, 0.0126, 0.0127, 0.0127,\n",
       "        0.0128, 0.0129, 0.0130, 0.0131, 0.0132, 0.0133, 0.0134, 0.0135, 0.0136,\n",
       "        0.0137, 0.0137, 0.0138, 0.0139, 0.0140, 0.0141, 0.0142, 0.0143, 0.0144,\n",
       "        0.0145, 0.0146, 0.0147, 0.0148, 0.0149, 0.0150, 0.0151, 0.0152, 0.0154,\n",
       "        0.0155, 0.0156, 0.0157, 0.0158, 0.0159, 0.0160, 0.0161, 0.0162, 0.0163,\n",
       "        0.0165, 0.0166, 0.0167, 0.0168, 0.0169, 0.0170, 0.0171, 0.0173, 0.0174,\n",
       "        0.0175, 0.0176, 0.0178, 0.0179, 0.0180, 0.0181, 0.0182, 0.0184, 0.0185,\n",
       "        0.0186, 0.0188, 0.0189, 0.0190, 0.0192, 0.0193, 0.0194, 0.0196, 0.0197,\n",
       "        0.0198, 0.0200, 0.0201, 0.0202, 0.0204, 0.0205, 0.0207, 0.0208, 0.0210,\n",
       "        0.0211, 0.0212, 0.0214, 0.0215, 0.0217, 0.0218, 0.0220, 0.0221, 0.0223,\n",
       "        0.0225, 0.0226, 0.0228, 0.0229, 0.0231, 0.0232, 0.0234, 0.0236, 0.0237,\n",
       "        0.0239, 0.0241, 0.0242, 0.0244, 0.0246, 0.0247, 0.0249, 0.0251, 0.0253,\n",
       "        0.0254, 0.0256, 0.0258, 0.0260, 0.0261, 0.0263, 0.0265, 0.0267, 0.0269,\n",
       "        0.0271, 0.0273, 0.0274, 0.0276, 0.0278, 0.0280, 0.0282, 0.0284, 0.0286,\n",
       "        0.0288, 0.0290, 0.0292, 0.0294, 0.0296, 0.0298, 0.0300, 0.0302, 0.0304,\n",
       "        0.0307, 0.0309, 0.0311, 0.0313, 0.0315, 0.0317, 0.0320, 0.0322, 0.0324,\n",
       "        0.0326, 0.0328, 0.0331, 0.0333, 0.0335, 0.0338, 0.0340, 0.0342, 0.0345,\n",
       "        0.0347, 0.0350, 0.0352, 0.0354, 0.0357, 0.0359, 0.0362, 0.0364, 0.0367,\n",
       "        0.0369, 0.0372, 0.0375, 0.0377, 0.0380, 0.0382, 0.0385, 0.0388, 0.0390,\n",
       "        0.0393, 0.0396, 0.0399, 0.0401, 0.0404, 0.0407, 0.0410, 0.0413, 0.0416,\n",
       "        0.0418, 0.0421, 0.0424, 0.0427, 0.0430, 0.0433, 0.0436, 0.0439, 0.0442,\n",
       "        0.0445, 0.0448, 0.0451, 0.0455, 0.0458, 0.0461, 0.0464, 0.0467, 0.0471,\n",
       "        0.0474, 0.0477, 0.0480, 0.0484, 0.0487, 0.0491, 0.0494, 0.0497, 0.0501,\n",
       "        0.0504, 0.0508, 0.0511, 0.0515, 0.0518, 0.0522, 0.0526, 0.0529, 0.0533,\n",
       "        0.0537, 0.0540, 0.0544, 0.0548, 0.0552, 0.0556, 0.0559, 0.0563, 0.0567,\n",
       "        0.0571, 0.0575, 0.0579, 0.0583, 0.0587, 0.0591, 0.0595, 0.0599, 0.0604,\n",
       "        0.0608, 0.0612, 0.0616, 0.0621, 0.0625, 0.0629, 0.0634, 0.0638, 0.0642,\n",
       "        0.0647, 0.0651, 0.0656, 0.0660, 0.0665, 0.0670, 0.0674, 0.0679, 0.0684,\n",
       "        0.0688, 0.0693, 0.0698, 0.0703, 0.0708, 0.0713, 0.0718, 0.0723, 0.0728,\n",
       "        0.0733, 0.0738, 0.0743, 0.0748, 0.0753, 0.0758, 0.0764, 0.0769, 0.0774,\n",
       "        0.0780, 0.0785, 0.0790, 0.0796, 0.0802, 0.0807, 0.0813, 0.0818, 0.0824,\n",
       "        0.0830, 0.0835, 0.0841, 0.0847, 0.0853, 0.0859, 0.0865, 0.0871, 0.0877,\n",
       "        0.0883, 0.0889, 0.0895, 0.0901, 0.0908, 0.0914, 0.0920, 0.0927, 0.0933,\n",
       "        0.0940, 0.0946, 0.0953, 0.0959, 0.0966, 0.0973, 0.0979, 0.0986, 0.0993,\n",
       "        0.1000, 0.1007, 0.1014, 0.1021, 0.1028, 0.1035, 0.1042, 0.1050, 0.1057,\n",
       "        0.1064, 0.1072, 0.1079, 0.1087, 0.1094, 0.1102, 0.1109, 0.1117, 0.1125,\n",
       "        0.1133, 0.1140, 0.1148, 0.1156, 0.1164, 0.1172, 0.1181, 0.1189, 0.1197,\n",
       "        0.1205, 0.1214, 0.1222, 0.1231, 0.1239, 0.1248, 0.1256, 0.1265, 0.1274,\n",
       "        0.1283, 0.1292, 0.1301, 0.1310, 0.1319, 0.1328, 0.1337, 0.1346, 0.1356,\n",
       "        0.1365, 0.1374, 0.1384, 0.1394, 0.1403, 0.1413, 0.1423, 0.1433, 0.1443,\n",
       "        0.1453, 0.1463, 0.1473, 0.1483, 0.1493, 0.1504, 0.1514, 0.1525, 0.1535,\n",
       "        0.1546, 0.1557, 0.1567, 0.1578, 0.1589, 0.1600, 0.1611, 0.1623, 0.1634,\n",
       "        0.1645, 0.1657, 0.1668, 0.1680, 0.1691, 0.1703, 0.1715, 0.1727, 0.1739,\n",
       "        0.1751, 0.1763, 0.1775, 0.1788, 0.1800, 0.1812, 0.1825, 0.1838, 0.1850,\n",
       "        0.1863, 0.1876, 0.1889, 0.1902, 0.1916, 0.1929, 0.1942, 0.1956, 0.1969,\n",
       "        0.1983, 0.1997, 0.2010, 0.2024, 0.2038, 0.2053, 0.2067, 0.2081, 0.2096,\n",
       "        0.2110, 0.2125, 0.2140, 0.2154, 0.2169, 0.2184, 0.2200, 0.2215, 0.2230,\n",
       "        0.2246, 0.2261, 0.2277, 0.2293, 0.2309, 0.2325, 0.2341, 0.2357, 0.2373,\n",
       "        0.2390, 0.2406, 0.2423, 0.2440, 0.2457, 0.2474, 0.2491, 0.2508, 0.2526,\n",
       "        0.2543, 0.2561, 0.2579, 0.2597, 0.2615, 0.2633, 0.2651, 0.2669, 0.2688,\n",
       "        0.2707, 0.2725, 0.2744, 0.2763, 0.2783, 0.2802, 0.2821, 0.2841, 0.2861,\n",
       "        0.2880, 0.2900, 0.2921, 0.2941, 0.2961, 0.2982, 0.3002, 0.3023, 0.3044,\n",
       "        0.3065, 0.3087, 0.3108, 0.3130, 0.3151, 0.3173, 0.3195, 0.3217, 0.3240,\n",
       "        0.3262, 0.3285, 0.3308, 0.3331, 0.3354, 0.3377, 0.3400, 0.3424, 0.3448,\n",
       "        0.3472, 0.3496, 0.3520, 0.3544, 0.3569, 0.3594, 0.3619, 0.3644, 0.3669,\n",
       "        0.3695, 0.3720, 0.3746, 0.3772, 0.3798, 0.3825, 0.3851, 0.3878, 0.3905,\n",
       "        0.3932, 0.3959, 0.3987, 0.4014, 0.4042, 0.4070, 0.4098, 0.4127, 0.4155,\n",
       "        0.4184, 0.4213, 0.4243, 0.4272, 0.4302, 0.4331, 0.4362, 0.4392, 0.4422,\n",
       "        0.4453, 0.4484, 0.4515, 0.4546, 0.4578, 0.4610, 0.4642, 0.4674, 0.4706,\n",
       "        0.4739, 0.4772, 0.4805, 0.4838, 0.4872, 0.4906, 0.4940, 0.4974, 0.5008,\n",
       "        0.5043, 0.5078, 0.5113, 0.5149, 0.5185, 0.5221, 0.5257, 0.5293, 0.5330,\n",
       "        0.5367, 0.5404, 0.5442, 0.5479, 0.5517, 0.5556, 0.5594, 0.5633, 0.5672,\n",
       "        0.5712, 0.5751, 0.5791, 0.5831, 0.5872, 0.5913, 0.5954, 0.5995, 0.6036,\n",
       "        0.6078, 0.6120, 0.6163, 0.6206, 0.6249, 0.6292, 0.6336, 0.6380, 0.6424,\n",
       "        0.6469, 0.6513, 0.6559, 0.6604, 0.6650, 0.6696, 0.6743, 0.6789, 0.6837,\n",
       "        0.6884, 0.6932, 0.6980, 0.7028, 0.7077, 0.7126, 0.7176, 0.7225, 0.7275,\n",
       "        0.7326, 0.7377, 0.7428, 0.7480, 0.7531, 0.7584, 0.7636, 0.7689, 0.7743,\n",
       "        0.7796, 0.7850, 0.7905, 0.7960, 0.8015, 0.8071, 0.8127, 0.8183, 0.8240,\n",
       "        0.8297, 0.8355, 0.8412, 0.8471, 0.8530, 0.8589, 0.8648, 0.8708, 0.8769,\n",
       "        0.8830, 0.8891, 0.8953, 0.9015, 0.9077, 0.9140, 0.9204, 0.9268, 0.9332,\n",
       "        0.9397, 0.9462, 0.9528, 0.9594, 0.9660, 0.9727, 0.9795, 0.9863, 0.9931,\n",
       "        1.0000])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How determine a good learning rate?\n",
    "# Create an matrix (linear space) of 1000 numbers (1000 possible learning rates) between 0.001 (When loss decreases but at a slow pace) and 1 (When loss starts to burst)\n",
    "lre = torch.linspace(-3, 0, 1000) # Learning rate exponent \n",
    "lrs = 10**lre # 10**-3 will be 0.001 and 10**0 will be 1\n",
    "lrs # Exponentially spaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cd10b379-30e9-4502-bdfe-ecbc22c0afba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.611328125\n",
      "17.767536163330078\n",
      "23.115434646606445\n",
      "21.670909881591797\n",
      "19.7050838470459\n",
      "21.132583618164062\n",
      "19.725933074951172\n",
      "17.193397521972656\n",
      "15.98493480682373\n",
      "19.611289978027344\n",
      "18.931562423706055\n",
      "18.081636428833008\n",
      "18.811193466186523\n",
      "20.365774154663086\n",
      "17.014699935913086\n",
      "18.298093795776367\n",
      "17.817638397216797\n",
      "18.2940673828125\n",
      "18.758708953857422\n",
      "17.704893112182617\n",
      "16.57598114013672\n",
      "18.20486831665039\n",
      "18.3070068359375\n",
      "20.93640899658203\n",
      "17.850994110107422\n",
      "18.244962692260742\n",
      "20.98422622680664\n",
      "20.164051055908203\n",
      "17.320859909057617\n",
      "15.259566307067871\n",
      "21.10274887084961\n",
      "19.45884895324707\n",
      "21.04018783569336\n",
      "16.74399757385254\n",
      "20.332456588745117\n",
      "19.406147003173828\n",
      "18.60762596130371\n",
      "19.258657455444336\n",
      "17.010644912719727\n",
      "20.001617431640625\n",
      "20.08252716064453\n",
      "18.99116325378418\n",
      "18.876998901367188\n",
      "17.59842872619629\n",
      "16.439250946044922\n",
      "19.93418312072754\n",
      "19.245454788208008\n",
      "16.55605697631836\n",
      "19.902008056640625\n",
      "18.575849533081055\n",
      "17.313602447509766\n",
      "18.500974655151367\n",
      "17.377574920654297\n",
      "16.610885620117188\n",
      "15.967408180236816\n",
      "16.53255271911621\n",
      "16.76338768005371\n",
      "15.749157905578613\n",
      "16.612682342529297\n",
      "17.44635581970215\n",
      "17.53033447265625\n",
      "19.414426803588867\n",
      "20.06468963623047\n",
      "17.991601943969727\n",
      "18.029956817626953\n",
      "16.930761337280273\n",
      "17.951711654663086\n",
      "19.20038414001465\n",
      "20.30782127380371\n",
      "16.68183708190918\n",
      "17.669849395751953\n",
      "17.054101943969727\n",
      "18.832834243774414\n",
      "18.623504638671875\n",
      "17.41848373413086\n",
      "16.13364028930664\n",
      "17.44392967224121\n",
      "17.85384750366211\n",
      "17.250595092773438\n",
      "16.98614501953125\n",
      "19.025440216064453\n",
      "15.022283554077148\n",
      "16.45275115966797\n",
      "19.6647891998291\n",
      "16.668806076049805\n",
      "18.101465225219727\n",
      "13.043550491333008\n",
      "18.802900314331055\n",
      "18.825984954833984\n",
      "16.282888412475586\n",
      "16.331890106201172\n",
      "18.080081939697266\n",
      "16.477163314819336\n",
      "17.92957878112793\n",
      "16.724220275878906\n",
      "17.14390754699707\n",
      "14.412353515625\n",
      "15.82905101776123\n",
      "16.2128963470459\n",
      "15.576345443725586\n",
      "13.752264976501465\n",
      "13.599517822265625\n",
      "15.7496337890625\n",
      "16.90587043762207\n",
      "19.810300827026367\n",
      "17.285755157470703\n",
      "14.729192733764648\n",
      "16.76742935180664\n",
      "17.85080909729004\n",
      "16.682003021240234\n",
      "16.901704788208008\n",
      "15.414124488830566\n",
      "16.225601196289062\n",
      "16.340660095214844\n",
      "13.71770191192627\n",
      "16.51317024230957\n",
      "14.45594310760498\n",
      "14.596824645996094\n",
      "14.420390129089355\n",
      "15.05693531036377\n",
      "18.449865341186523\n",
      "15.17254638671875\n",
      "15.70726490020752\n",
      "18.438873291015625\n",
      "17.377866744995117\n",
      "15.537717819213867\n",
      "17.428993225097656\n",
      "16.71050453186035\n",
      "16.074024200439453\n",
      "17.66776466369629\n",
      "14.932765007019043\n",
      "12.683098793029785\n",
      "13.163360595703125\n",
      "15.51505184173584\n",
      "16.69551658630371\n",
      "15.032912254333496\n",
      "16.025733947753906\n",
      "17.84125328063965\n",
      "13.791653633117676\n",
      "16.27500343322754\n",
      "15.509010314941406\n",
      "14.305554389953613\n",
      "16.7181339263916\n",
      "18.446264266967773\n",
      "18.259422302246094\n",
      "16.86526870727539\n",
      "14.28609848022461\n",
      "14.054644584655762\n",
      "15.388471603393555\n",
      "13.72233772277832\n",
      "15.408008575439453\n",
      "15.298242568969727\n",
      "13.499385833740234\n",
      "12.928802490234375\n",
      "12.276810646057129\n",
      "15.279976844787598\n",
      "16.24513053894043\n",
      "13.760316848754883\n",
      "14.618429183959961\n",
      "16.22218894958496\n",
      "15.845473289489746\n",
      "13.137946128845215\n",
      "14.107075691223145\n",
      "14.289436340332031\n",
      "13.276798248291016\n",
      "16.05815887451172\n",
      "13.923145294189453\n",
      "14.482927322387695\n",
      "15.396342277526855\n",
      "14.86855697631836\n",
      "16.655601501464844\n",
      "15.400079727172852\n",
      "15.702088356018066\n",
      "13.256787300109863\n",
      "15.900115013122559\n",
      "15.744755744934082\n",
      "12.3969087600708\n",
      "16.76105308532715\n",
      "18.018503189086914\n",
      "15.326313972473145\n",
      "14.617186546325684\n",
      "11.663800239562988\n",
      "14.668336868286133\n",
      "14.01709270477295\n",
      "17.59717559814453\n",
      "17.874685287475586\n",
      "16.14241600036621\n",
      "14.43690013885498\n",
      "12.4829683303833\n",
      "14.534309387207031\n",
      "13.093979835510254\n",
      "14.57042407989502\n",
      "12.399599075317383\n",
      "13.058857917785645\n",
      "14.190254211425781\n",
      "17.561052322387695\n",
      "12.278008460998535\n",
      "17.13240623474121\n",
      "16.381389617919922\n",
      "15.181516647338867\n",
      "13.886368751525879\n",
      "13.627367973327637\n",
      "13.958340644836426\n",
      "15.206690788269043\n",
      "16.358495712280273\n",
      "13.502872467041016\n",
      "13.777420043945312\n",
      "13.732962608337402\n",
      "15.766511917114258\n",
      "15.169229507446289\n",
      "13.689088821411133\n",
      "14.378992080688477\n",
      "14.290040969848633\n",
      "12.935731887817383\n",
      "13.595963478088379\n",
      "10.751680374145508\n",
      "14.374574661254883\n",
      "10.874129295349121\n",
      "14.969331741333008\n",
      "16.209335327148438\n",
      "13.048155784606934\n",
      "14.203262329101562\n",
      "12.44343376159668\n",
      "13.096445083618164\n",
      "15.475817680358887\n",
      "13.42416000366211\n",
      "12.82335090637207\n",
      "12.876473426818848\n",
      "13.589353561401367\n",
      "11.940057754516602\n",
      "11.760104179382324\n",
      "14.059652328491211\n",
      "11.294798851013184\n",
      "14.162971496582031\n",
      "12.548600196838379\n",
      "14.246374130249023\n",
      "15.128600120544434\n",
      "14.91060733795166\n",
      "12.028508186340332\n",
      "13.69058895111084\n",
      "15.236454010009766\n",
      "10.809396743774414\n",
      "9.948688507080078\n",
      "11.010431289672852\n",
      "12.234557151794434\n",
      "12.962543487548828\n",
      "12.191137313842773\n",
      "10.811527252197266\n",
      "10.742828369140625\n",
      "13.179041862487793\n",
      "13.083666801452637\n",
      "11.299334526062012\n",
      "11.290102005004883\n",
      "11.092350959777832\n",
      "14.946942329406738\n",
      "11.087615013122559\n",
      "9.953143119812012\n",
      "11.108339309692383\n",
      "12.685796737670898\n",
      "13.449263572692871\n",
      "10.234001159667969\n",
      "11.894158363342285\n",
      "10.318331718444824\n",
      "11.650103569030762\n",
      "10.074748039245605\n",
      "13.482707023620605\n",
      "12.791163444519043\n",
      "11.507258415222168\n",
      "14.2972993850708\n",
      "9.823617935180664\n",
      "10.331649780273438\n",
      "10.425464630126953\n",
      "13.703354835510254\n",
      "11.292049407958984\n",
      "12.250237464904785\n",
      "13.084664344787598\n",
      "9.579354286193848\n",
      "11.567960739135742\n",
      "13.05495548248291\n",
      "11.16825008392334\n",
      "10.418262481689453\n",
      "11.535430908203125\n",
      "12.452476501464844\n",
      "10.649393081665039\n",
      "12.603182792663574\n",
      "9.095508575439453\n",
      "12.440762519836426\n",
      "12.15817928314209\n",
      "10.938315391540527\n",
      "10.346582412719727\n",
      "9.75310230255127\n",
      "10.15543270111084\n",
      "9.784906387329102\n",
      "11.648517608642578\n",
      "12.341193199157715\n",
      "9.478796005249023\n",
      "10.882034301757812\n",
      "13.61843204498291\n",
      "11.208901405334473\n",
      "7.507067680358887\n",
      "11.513079643249512\n",
      "10.991199493408203\n",
      "8.557490348815918\n",
      "10.022905349731445\n",
      "9.073389053344727\n",
      "9.622272491455078\n",
      "9.894644737243652\n",
      "10.012168884277344\n",
      "8.607426643371582\n",
      "9.287178993225098\n",
      "11.177938461303711\n",
      "13.519495010375977\n",
      "10.119954109191895\n",
      "12.317373275756836\n",
      "7.29680061340332\n",
      "11.84024429321289\n",
      "10.606624603271484\n",
      "10.393983840942383\n",
      "10.356358528137207\n",
      "9.108915328979492\n",
      "9.48698616027832\n",
      "11.953655242919922\n",
      "10.0061616897583\n",
      "11.30268669128418\n",
      "10.381576538085938\n",
      "8.147965431213379\n",
      "8.55633544921875\n",
      "11.1993408203125\n",
      "10.750038146972656\n",
      "12.188485145568848\n",
      "8.09498119354248\n",
      "10.893376350402832\n",
      "11.675154685974121\n",
      "9.839003562927246\n",
      "9.728224754333496\n",
      "9.086281776428223\n",
      "7.335202217102051\n",
      "8.0067138671875\n",
      "11.817770004272461\n",
      "9.91334342956543\n",
      "10.276006698608398\n",
      "8.206255912780762\n",
      "9.29052448272705\n",
      "11.408515930175781\n",
      "10.907950401306152\n",
      "11.129568099975586\n",
      "12.843887329101562\n",
      "9.23939037322998\n",
      "11.705780982971191\n",
      "9.910122871398926\n",
      "8.963678359985352\n",
      "9.49565601348877\n",
      "8.355751991271973\n",
      "9.348999977111816\n",
      "9.823678016662598\n",
      "8.88793659210205\n",
      "10.425545692443848\n",
      "8.1058988571167\n",
      "11.866039276123047\n",
      "9.842718124389648\n",
      "8.163288116455078\n",
      "8.76578426361084\n",
      "9.305258750915527\n",
      "8.186400413513184\n",
      "9.435561180114746\n",
      "8.968610763549805\n",
      "8.582195281982422\n",
      "8.403206825256348\n",
      "8.107963562011719\n",
      "9.385359764099121\n",
      "8.753604888916016\n",
      "8.72230052947998\n",
      "10.52587604522705\n",
      "8.367277145385742\n",
      "9.061734199523926\n",
      "9.810190200805664\n",
      "7.088253498077393\n",
      "9.55705451965332\n",
      "11.021883010864258\n",
      "8.031023979187012\n",
      "7.691132545471191\n",
      "7.749887943267822\n",
      "8.297929763793945\n",
      "9.002537727355957\n",
      "8.19502067565918\n",
      "7.7129716873168945\n",
      "9.516661643981934\n",
      "8.76560115814209\n",
      "9.129729270935059\n",
      "9.579785346984863\n",
      "7.234193801879883\n",
      "7.252667427062988\n",
      "8.541337966918945\n",
      "7.309537410736084\n",
      "10.073201179504395\n",
      "8.63316822052002\n",
      "8.692301750183105\n",
      "8.088802337646484\n",
      "9.20824146270752\n",
      "5.8663105964660645\n",
      "5.9252519607543945\n",
      "8.530673027038574\n",
      "8.87657356262207\n",
      "6.941271781921387\n",
      "7.272482872009277\n",
      "8.904237747192383\n",
      "8.352775573730469\n",
      "7.299280166625977\n",
      "6.533027648925781\n",
      "6.87067985534668\n",
      "6.997208595275879\n",
      "6.633702278137207\n",
      "8.873983383178711\n",
      "9.438532829284668\n",
      "9.768763542175293\n",
      "7.530399322509766\n",
      "9.086299896240234\n",
      "9.131622314453125\n",
      "6.767035961151123\n",
      "7.389681816101074\n",
      "6.929438591003418\n",
      "8.178888320922852\n",
      "9.607560157775879\n",
      "7.268725872039795\n",
      "8.523950576782227\n",
      "8.141057968139648\n",
      "7.536422252655029\n",
      "8.668683052062988\n",
      "7.737354755401611\n",
      "6.844742298126221\n",
      "7.09881067276001\n",
      "6.459590435028076\n",
      "8.50132942199707\n",
      "7.451815128326416\n",
      "7.992978572845459\n",
      "5.897587776184082\n",
      "6.966641426086426\n",
      "6.289462566375732\n",
      "6.774393558502197\n",
      "7.4801411628723145\n",
      "9.470123291015625\n",
      "5.319653511047363\n",
      "7.685760498046875\n",
      "4.075367450714111\n",
      "5.919536590576172\n",
      "6.084640979766846\n",
      "7.091085910797119\n",
      "7.038312911987305\n",
      "8.807075500488281\n",
      "5.810422420501709\n",
      "7.209746360778809\n",
      "7.27494478225708\n",
      "6.1334309577941895\n",
      "7.517580509185791\n",
      "8.408567428588867\n",
      "7.127126216888428\n",
      "8.310129165649414\n",
      "6.040768623352051\n",
      "5.495812892913818\n",
      "6.038758277893066\n",
      "6.105738639831543\n",
      "6.927518844604492\n",
      "7.238379001617432\n",
      "6.976246356964111\n",
      "5.2646894454956055\n",
      "4.593191146850586\n",
      "5.018383026123047\n",
      "7.1453399658203125\n",
      "6.185591697692871\n",
      "5.594015121459961\n",
      "7.881969928741455\n",
      "8.25730037689209\n",
      "5.161339282989502\n",
      "6.137485980987549\n",
      "7.499630451202393\n",
      "5.315673828125\n",
      "4.776252746582031\n",
      "6.400079250335693\n",
      "6.256616115570068\n",
      "4.847609519958496\n",
      "6.262917518615723\n",
      "5.764100074768066\n",
      "6.441140174865723\n",
      "4.562678337097168\n",
      "5.838088035583496\n",
      "7.2313408851623535\n",
      "6.131582260131836\n",
      "5.182438373565674\n",
      "7.5282063484191895\n",
      "6.023227691650391\n",
      "5.875815391540527\n",
      "4.346658229827881\n",
      "4.5487165451049805\n",
      "4.407459259033203\n",
      "5.722019672393799\n",
      "5.438461780548096\n",
      "3.5341432094573975\n",
      "5.249074935913086\n",
      "6.200826168060303\n",
      "5.379970550537109\n",
      "4.649435520172119\n",
      "5.106847286224365\n",
      "5.324408531188965\n",
      "4.1114182472229\n",
      "4.6369476318359375\n",
      "3.7792129516601562\n",
      "3.8282535076141357\n",
      "3.9223461151123047\n",
      "5.525158405303955\n",
      "4.3957390785217285\n",
      "5.039926528930664\n",
      "5.695961952209473\n",
      "5.816729545593262\n",
      "5.264459609985352\n",
      "5.667156219482422\n",
      "4.434756278991699\n",
      "4.977132320404053\n",
      "4.611456394195557\n",
      "4.009191036224365\n",
      "5.429199695587158\n",
      "5.736825466156006\n",
      "4.173170566558838\n",
      "4.824768543243408\n",
      "5.270089149475098\n",
      "5.628945827484131\n",
      "3.469482183456421\n",
      "5.6920647621154785\n",
      "4.2656965255737305\n",
      "4.945101737976074\n",
      "4.7528886795043945\n",
      "4.74647855758667\n",
      "3.801330089569092\n",
      "4.87331485748291\n",
      "4.774423122406006\n",
      "4.743813991546631\n",
      "4.441451549530029\n",
      "4.735416889190674\n",
      "4.224179267883301\n",
      "3.700212001800537\n",
      "4.294844627380371\n",
      "5.345515727996826\n",
      "3.571521282196045\n",
      "4.728123188018799\n",
      "3.6905972957611084\n",
      "3.697011947631836\n",
      "5.077097415924072\n",
      "5.444340229034424\n",
      "4.058139324188232\n",
      "4.793383598327637\n",
      "3.3013134002685547\n",
      "3.4576358795166016\n",
      "4.207733631134033\n",
      "4.921144485473633\n",
      "4.577362060546875\n",
      "5.533949851989746\n",
      "4.1084885597229\n",
      "3.332362651824951\n",
      "3.62739634513855\n",
      "3.915994167327881\n",
      "4.501053810119629\n",
      "5.1504011154174805\n",
      "3.761763095855713\n",
      "6.151395320892334\n",
      "3.8422281742095947\n",
      "4.457144260406494\n",
      "4.159154891967773\n",
      "3.566864013671875\n",
      "4.285024166107178\n",
      "3.646926164627075\n",
      "3.6891367435455322\n",
      "4.3466877937316895\n",
      "3.7088582515716553\n",
      "3.904749870300293\n",
      "3.8410537242889404\n",
      "5.245121955871582\n",
      "4.245830059051514\n",
      "3.8112430572509766\n",
      "3.1972644329071045\n",
      "4.538957595825195\n",
      "4.507936000823975\n",
      "4.41500997543335\n",
      "4.513865947723389\n",
      "3.9149630069732666\n",
      "3.4315454959869385\n",
      "3.2666077613830566\n",
      "4.512600421905518\n",
      "4.609901428222656\n",
      "4.159539699554443\n",
      "4.203895092010498\n",
      "4.831700801849365\n",
      "4.2068610191345215\n",
      "4.189352512359619\n",
      "4.213104248046875\n",
      "4.897179126739502\n",
      "4.09975004196167\n",
      "3.6508054733276367\n",
      "3.0276663303375244\n",
      "3.916546106338501\n",
      "3.1532585620880127\n",
      "3.4723665714263916\n",
      "4.035224437713623\n",
      "3.698219060897827\n",
      "3.3900365829467773\n",
      "4.656062126159668\n",
      "4.792466163635254\n",
      "4.138199329376221\n",
      "3.552814483642578\n",
      "3.3849892616271973\n",
      "4.530372142791748\n",
      "3.6499831676483154\n",
      "3.563030481338501\n",
      "4.066023826599121\n",
      "2.8882980346679688\n",
      "4.059221267700195\n",
      "4.579761028289795\n",
      "3.6639931201934814\n",
      "3.1780550479888916\n",
      "3.4356889724731445\n",
      "4.080757141113281\n",
      "3.028927803039551\n",
      "4.135476112365723\n",
      "2.977814197540283\n",
      "3.999485969543457\n",
      "3.069791793823242\n",
      "3.338024377822876\n",
      "3.335346221923828\n",
      "4.528201103210449\n",
      "3.617621421813965\n",
      "4.656489372253418\n",
      "3.1160888671875\n",
      "3.986560344696045\n",
      "3.6870486736297607\n",
      "3.930450439453125\n",
      "3.776514768600464\n",
      "3.5183870792388916\n",
      "2.9980688095092773\n",
      "2.811852216720581\n",
      "3.352881908416748\n",
      "3.037764549255371\n",
      "2.9838602542877197\n",
      "3.188755989074707\n",
      "3.3665390014648438\n",
      "3.266756296157837\n",
      "3.1242501735687256\n",
      "3.4071338176727295\n",
      "3.5540385246276855\n",
      "3.642007350921631\n",
      "2.8845601081848145\n",
      "2.77411150932312\n",
      "3.671036720275879\n",
      "3.0719563961029053\n",
      "3.381702423095703\n",
      "3.1580116748809814\n",
      "3.332111120223999\n",
      "3.684802532196045\n",
      "3.3415586948394775\n",
      "3.7238786220550537\n",
      "3.336414337158203\n",
      "3.1541996002197266\n",
      "3.2789723873138428\n",
      "2.6919009685516357\n",
      "2.9372618198394775\n",
      "3.052248001098633\n",
      "3.7239341735839844\n",
      "3.4067463874816895\n",
      "3.017698049545288\n",
      "3.4257497787475586\n",
      "3.192545175552368\n",
      "3.6225249767303467\n",
      "3.2857894897460938\n",
      "3.2540464401245117\n",
      "2.921654462814331\n",
      "3.045057773590088\n",
      "2.6021485328674316\n",
      "3.5195975303649902\n",
      "2.8307104110717773\n",
      "2.5343759059906006\n",
      "3.4761157035827637\n",
      "2.9097402095794678\n",
      "2.848855972290039\n",
      "3.255402088165283\n",
      "2.5473642349243164\n",
      "3.0278658866882324\n",
      "2.2292075157165527\n",
      "2.9953341484069824\n",
      "2.9826149940490723\n",
      "3.8710148334503174\n",
      "2.9451959133148193\n",
      "2.9260082244873047\n",
      "3.171438694000244\n",
      "3.1970510482788086\n",
      "3.3458735942840576\n",
      "3.1558966636657715\n",
      "3.0200514793395996\n",
      "3.378415822982788\n",
      "2.6229770183563232\n",
      "3.1192502975463867\n",
      "3.033435344696045\n",
      "3.284499406814575\n",
      "2.986362934112549\n",
      "3.049232006072998\n",
      "2.977276563644409\n",
      "2.870929718017578\n",
      "2.659980297088623\n",
      "3.0754306316375732\n",
      "3.850709915161133\n",
      "3.0489084720611572\n",
      "3.5170063972473145\n",
      "3.488095998764038\n",
      "2.867464780807495\n",
      "2.7371649742126465\n",
      "3.1476259231567383\n",
      "3.555987596511841\n",
      "2.840613603591919\n",
      "3.1383936405181885\n",
      "3.2124645709991455\n",
      "3.0837314128875732\n",
      "3.177851676940918\n",
      "2.5928030014038086\n",
      "2.604250907897949\n",
      "2.863299608230591\n",
      "3.6719679832458496\n",
      "3.37542986869812\n",
      "3.0577940940856934\n",
      "3.706808567047119\n",
      "3.4165122509002686\n",
      "3.097778797149658\n",
      "3.1634702682495117\n",
      "3.2274765968322754\n",
      "3.6209280490875244\n",
      "2.764268159866333\n",
      "3.2387497425079346\n",
      "3.7461395263671875\n",
      "3.0273590087890625\n",
      "3.0071349143981934\n",
      "3.1745500564575195\n",
      "2.7053802013397217\n",
      "2.7341980934143066\n",
      "2.9897899627685547\n",
      "2.7923948764801025\n",
      "2.998187303543091\n",
      "3.6123578548431396\n",
      "3.16522216796875\n",
      "2.969115734100342\n",
      "3.447521448135376\n",
      "4.097039699554443\n",
      "2.5587918758392334\n",
      "2.617981195449829\n",
      "3.423823595046997\n",
      "2.5963993072509766\n",
      "3.4020931720733643\n",
      "2.906708002090454\n",
      "3.105999708175659\n",
      "2.8189103603363037\n",
      "3.0396273136138916\n",
      "2.5817573070526123\n",
      "3.004610538482666\n",
      "3.9870545864105225\n",
      "2.9389474391937256\n",
      "3.3019256591796875\n",
      "3.2447237968444824\n",
      "3.1746177673339844\n",
      "3.7113399505615234\n",
      "3.2988791465759277\n",
      "3.6236822605133057\n",
      "2.3419814109802246\n",
      "2.774418592453003\n",
      "2.9075238704681396\n",
      "3.283207416534424\n",
      "3.37542724609375\n",
      "3.425529718399048\n",
      "4.221156120300293\n",
      "3.1877198219299316\n",
      "3.3682684898376465\n",
      "3.521454095840454\n",
      "3.21659517288208\n",
      "3.68861985206604\n",
      "3.604142189025879\n",
      "3.312891721725464\n",
      "3.5407609939575195\n",
      "3.242198944091797\n",
      "3.0669491291046143\n",
      "2.591024398803711\n",
      "3.3892934322357178\n",
      "4.353494644165039\n",
      "4.232124328613281\n",
      "3.641878366470337\n",
      "4.846114158630371\n",
      "3.2994093894958496\n",
      "3.356609344482422\n",
      "3.398698329925537\n",
      "3.164494514465332\n",
      "3.244208574295044\n",
      "2.582505941390991\n",
      "2.7554872035980225\n",
      "3.3959789276123047\n",
      "3.084289073944092\n",
      "3.5154404640197754\n",
      "4.144484519958496\n",
      "3.4958505630493164\n",
      "3.033043622970581\n",
      "3.158827543258667\n",
      "3.937246322631836\n",
      "3.481473207473755\n",
      "3.1774709224700928\n",
      "4.507720470428467\n",
      "3.386054039001465\n",
      "3.5854930877685547\n",
      "3.4444572925567627\n",
      "3.1396102905273438\n",
      "3.4239253997802734\n",
      "3.220278024673462\n",
      "3.4636712074279785\n",
      "4.14219331741333\n",
      "3.4094157218933105\n",
      "3.952888250350952\n",
      "2.62870192527771\n",
      "2.4349658489227295\n",
      "3.2401986122131348\n",
      "3.7309646606445312\n",
      "3.3794519901275635\n",
      "3.79937481880188\n",
      "3.4175219535827637\n",
      "3.6027915477752686\n",
      "3.9993996620178223\n",
      "4.349558353424072\n",
      "3.969881057739258\n",
      "4.187226295471191\n",
      "3.6842527389526367\n",
      "3.4253339767456055\n",
      "3.927067756652832\n",
      "3.703958749771118\n",
      "3.950312852859497\n",
      "3.852039098739624\n",
      "3.7537901401519775\n",
      "4.161771774291992\n",
      "3.599457263946533\n",
      "3.0558483600616455\n",
      "3.66536808013916\n",
      "3.621225118637085\n",
      "4.806344509124756\n",
      "5.046644687652588\n",
      "3.972519874572754\n",
      "4.16221284866333\n",
      "3.3685343265533447\n",
      "2.9237279891967773\n",
      "5.024844646453857\n",
      "3.47056245803833\n",
      "4.250846862792969\n",
      "2.7097060680389404\n",
      "3.4192521572113037\n",
      "3.9724955558776855\n",
      "4.312405109405518\n",
      "4.399214267730713\n",
      "3.8876137733459473\n",
      "3.8679842948913574\n",
      "3.522068738937378\n",
      "3.1111085414886475\n",
      "3.6217048168182373\n",
      "3.6974611282348633\n",
      "3.148904323577881\n",
      "2.9279587268829346\n",
      "3.381305456161499\n",
      "3.5908620357513428\n",
      "4.665445804595947\n",
      "5.081798553466797\n",
      "3.862179756164551\n",
      "4.063606262207031\n",
      "4.975341320037842\n",
      "4.406893730163574\n",
      "4.596571922302246\n",
      "3.2401723861694336\n",
      "4.783205986022949\n",
      "3.6496520042419434\n",
      "4.165107250213623\n",
      "3.4387850761413574\n",
      "3.8449575901031494\n",
      "4.5425543785095215\n",
      "4.538517951965332\n",
      "4.138350486755371\n",
      "4.72438383102417\n",
      "4.381353378295898\n",
      "4.430253505706787\n",
      "4.085100173950195\n",
      "4.487863063812256\n",
      "3.8689346313476562\n",
      "4.037130355834961\n",
      "4.621676921844482\n",
      "4.826233386993408\n",
      "4.034096717834473\n",
      "4.130059719085693\n",
      "4.221363544464111\n",
      "4.137467861175537\n",
      "4.886175155639648\n",
      "3.640347480773926\n",
      "4.007462501525879\n",
      "5.245603561401367\n",
      "6.013917922973633\n",
      "6.087913513183594\n",
      "6.2374162673950195\n",
      "5.17565393447876\n",
      "4.515269756317139\n",
      "4.884426593780518\n",
      "5.16819429397583\n",
      "4.1521711349487305\n",
      "4.35222864151001\n",
      "5.6456708908081055\n",
      "4.587376594543457\n",
      "4.520332336425781\n",
      "5.27402400970459\n",
      "7.343648910522461\n",
      "7.142712593078613\n",
      "5.916014194488525\n",
      "7.3332109451293945\n",
      "4.319550514221191\n",
      "4.875203609466553\n",
      "3.9271111488342285\n",
      "4.26945686340332\n",
      "4.122025489807129\n",
      "4.663464069366455\n",
      "4.646923065185547\n",
      "5.096287727355957\n",
      "5.16853666305542\n",
      "4.792318344116211\n",
      "4.5476274490356445\n",
      "4.934132099151611\n",
      "3.899960517883301\n",
      "5.253934860229492\n",
      "3.8339438438415527\n",
      "3.3506882190704346\n",
      "3.9551172256469727\n",
      "4.734062671661377\n",
      "4.093480587005615\n",
      "5.690366744995117\n",
      "5.153903961181641\n",
      "5.056943416595459\n",
      "6.374901294708252\n",
      "5.538093090057373\n",
      "5.141846656799316\n",
      "5.4836015701293945\n",
      "5.5390753746032715\n",
      "6.115057945251465\n",
      "6.592177867889404\n",
      "8.273680686950684\n",
      "4.994464874267578\n",
      "6.178645133972168\n",
      "4.049705505371094\n",
      "4.675453186035156\n",
      "6.684638500213623\n",
      "6.272758483886719\n",
      "5.937695026397705\n",
      "4.335744380950928\n",
      "4.3781280517578125\n",
      "6.250847339630127\n",
      "6.717866897583008\n",
      "5.698508262634277\n",
      "6.734145641326904\n",
      "5.064863204956055\n",
      "5.363193035125732\n",
      "5.136847019195557\n",
      "6.507082462310791\n",
      "5.775679588317871\n",
      "5.962278842926025\n",
      "8.017979621887207\n",
      "6.497536659240723\n",
      "5.150564193725586\n",
      "6.360506057739258\n",
      "6.881246089935303\n",
      "7.552270412445068\n",
      "8.332490921020508\n",
      "6.279541492462158\n",
      "5.68321418762207\n",
      "5.526957988739014\n",
      "6.478602409362793\n",
      "6.658536911010742\n",
      "7.373603343963623\n",
      "6.946004867553711\n",
      "8.010258674621582\n",
      "8.265484809875488\n",
      "6.216135025024414\n",
      "6.230860233306885\n",
      "5.347538948059082\n",
      "6.293585300445557\n",
      "12.776904106140137\n",
      "8.663241386413574\n",
      "11.847899436950684\n",
      "9.068366050720215\n",
      "8.446975708007812\n",
      "9.827621459960938\n",
      "7.158621311187744\n",
      "7.540249347686768\n",
      "7.661037921905518\n",
      "9.201213836669922\n",
      "7.848937034606934\n",
      "6.986225128173828\n",
      "7.412533760070801\n",
      "7.269004821777344\n",
      "5.156262397766113\n",
      "7.620918273925781\n",
      "6.235569477081299\n"
     ]
    }
   ],
   "source": [
    "# Reinitialisation for calculating lr\n",
    "g = torch.Generator().manual_seed(2147483647) # for same result reproducibility\n",
    "C = torch.randn((27, 2), generator=g)\n",
    "W1 = torch.randn((6, 100), generator = g)\n",
    "b1 = torch.randn(100, generator = g)\n",
    "W2 = torch.randn((100, 27), generator = g)\n",
    "b2 = torch.randn(27, generator = g)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "lri = [] # To track lrs used\n",
    "lossi = [] # To track losses that lr produced\n",
    "for i in range(1000):\n",
    "    ix = torch.randint(0, X.shape[0], (32,))\n",
    "    # FORWARD PASS\n",
    "    emb = C[X[ix]] # Embeddings (32, 3, 2)\n",
    "    # First layer will be 3 input embeddings\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # Second (hidden) Layer (32, 100)\n",
    "    logits = h @ W2 + b2 # Third Layer (32, 27)\n",
    "    loss = F.cross_entropy(logits, Y[ix])\n",
    "    print(loss.item())\n",
    "    # BACKWARD PASS\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    lr = lrs[i] # Changing lr with every iteration # Starting with very low 0.001 then going upto 1\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    # Track stats\n",
    "    lri.append(lre[i])\n",
    "    lossi.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8aa6ddc1-5114-4891-a80e-15521796d6be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x16797af60>]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot lri, lossi for visualisation\n",
    "plt.plot(lri, lossi)\n",
    "# The output shows that somewhere around -1.0 and -0.5 the loss is least.\n",
    "# So we will use lr as 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d4d3cb96-67af-4c0d-bfac-90eab4b95459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.414402723312378\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647) # for same result reproducibility\n",
    "C = torch.randn((27, 2), generator=g)\n",
    "W1 = torch.randn((6, 100), generator = g)\n",
    "b1 = torch.randn(100, generator = g)\n",
    "W2 = torch.randn((100, 27), generator = g)\n",
    "b2 = torch.randn(27, generator = g)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "for i in range(1000):\n",
    "    ix = torch.randint(0, X.shape[0], (32,))\n",
    "    # FORWARD PASS\n",
    "    emb = C[X[ix]] # Embeddings (32, 3, 2)\n",
    "    # First layer will be 3 input embeddings\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # Second (hidden) Layer (32, 100)\n",
    "    logits = h @ W2 + b2 # Third Layer (32, 27)\n",
    "    loss = F.cross_entropy(logits, Y[ix])\n",
    "    # BACKWARD PASS\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    lr = 0.1\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5714b5d0-0c2f-4905-803a-dd9c9bc52a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.978369951248169\n"
     ]
    }
   ],
   "source": [
    "# Learning rate decay - At later stages of training, we lower the learning rate to minimise gradient movement and keep on training the model\n",
    "# For example - We run the model for first 20000 epochs at lr = 0.1 then set lr = 0.01 for next 10000 epochs\n",
    "for i in range(1000):\n",
    "    ix = torch.randint(0, X.shape[0], (32,))\n",
    "    # FORWARD PASS\n",
    "    emb = C[X[ix]] # Embeddings (32, 3, 2)\n",
    "    # First layer will be 3 input embeddings\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # Second (hidden) Layer (32, 100)\n",
    "    logits = h @ W2 + b2 # Third Layer (32, 27)\n",
    "    loss = F.cross_entropy(logits, Y[ix])\n",
    "    # BACKWARD PASS\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    lr = 0.01 # Decayed lr\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3eeea6d-ee52-4461-a549-d4f4d14c47fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "When nueral net size is big, that is when there are a lot of params and the training data is less, the model learns to\n",
    "underfit. \n",
    "This means that the model will learn to memorise the data in training data and what output it should produce.\n",
    "Therefore the loss on training data will be much less but on test data will be much higher, therefore a bad model.\n",
    "\n",
    "To solve this, we split the training data into 3 parts, namely: \n",
    "- Training Split(Around 80%, used to train the model params), \n",
    "- Dev/Validation Split (Around 10%, used to train the hyperparameters), \n",
    "- Test Split(Around 10%, used to evaluate the performance of model. Used only when model is trained so that model doesn't overfit on test data\n",
    "as well).\n",
    "\"\"\"\n",
    "# Building dataset\n",
    "\n",
    "def build_dataset(words):\n",
    "    block_size = 3\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w+'.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xtr, Ytr = build_dataset(words[:n1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
